import base64
import copy
import json
import os
import re
import smtplib
import ssl
import time as systime
from datetime import date, datetime, time, timedelta
from email.mime.application import MIMEApplication
from email.mime.image import MIMEImage
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from itertools import islice

import certifi
import matplotlib.dates as mdates
import matplotlib.font_manager as fm
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pytz
import requests
import seaborn as sns
from backoff import expo, on_exception  # ì§€ìˆ˜ ë°±ì˜¤í”„ ì¶”ê°€
from bs4 import BeautifulSoup
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from googleapiclient.http import MediaFileUpload
from matplotlib.patches import Patch
from oauth2client.service_account import ServiceAccountCredentials

# í™˜ê²½ë³€ìˆ˜ ë¡œë”©
try:
    from dotenv import load_dotenv

    load_dotenv()  # .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    print("âœ… .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
except ImportError:
    print("âš ï¸ python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    print("   ì„¤ì¹˜ ë°©ë²•: pip install python-dotenv")

# ë¡œê¹… ì„¤ì •
import logging

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("pda_partner.log"), logging.StreamHandler()],
)
logger = logging.getLogger(__name__)

# SSL í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
os.environ["SSL_CERT_FILE"] = certifi.where()

# Global Variables - ë³´ì•ˆ ê°œì„ : í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
TEST_MODE = os.getenv("TEST_MODE", "True").lower() == "true"
DRIVE_FOLDER_ID = os.getenv(
    "DRIVE_FOLDER_ID", "1Gylm36vhtrl_yCHurZYGgeMlt5U0CliE"
)  # ìµœì¢… ë¦¬í¬íŠ¸(HTML), ê·¸ë˜í”„ ë“± ì €ì¥ìš©
JSON_DRIVE_FOLDER_ID = os.getenv(
    "JSON_DRIVE_FOLDER_ID", "13FdsniLHb4qKmn5M4-75H8SvgEyW2Ck1"
)  # JSON ë°ì´í„° ì €ì¥ìš©

# í™˜ê²½ë³€ìˆ˜ ë””ë²„ê¹… ë¡œê·¸
print(f"ğŸ” [DEBUG] DRIVE_FOLDER_ID í™˜ê²½ë³€ìˆ˜: '{DRIVE_FOLDER_ID}'")
print(f"ğŸ” [DEBUG] JSON_DRIVE_FOLDER_ID í™˜ê²½ë³€ìˆ˜: '{JSON_DRIVE_FOLDER_ID}'")
SMTP_SERVER = os.getenv("SMTP_SERVER", "smtp.gmail.com")
SMTP_PORT = int(os.getenv("SMTP_PORT", "587"))
# ì´ë©”ì¼ ì„¤ì • - ê¸°ì¡´ .env íŒŒì¼ í˜¸í™˜ì„± ì§€ì›
EMAIL_ADDRESS = os.getenv("EMAIL_ADDRESS") or os.getenv("SMTP_USER")
EMAIL_PASS = os.getenv("EMAIL_PASS") or os.getenv("SMTP_PASSWORD")
RECEIVER_EMAIL = os.getenv("RECEIVER_EMAIL")

# ì´ë©”ì¼ ì„¤ì • ê²€ì¦ (ì„ íƒì‚¬í•­)
email_configured = EMAIL_ADDRESS and EMAIL_PASS and RECEIVER_EMAIL
if not email_configured:
    print(f"âš ï¸ ì´ë©”ì¼ ì„¤ì • í™•ì¸ (ì„ íƒì‚¬í•­):")
    print(f"   EMAIL_ADDRESS/SMTP_USER: {'âœ…' if EMAIL_ADDRESS else 'âŒ'}")
    print(f"   EMAIL_PASS/SMTP_PASSWORD: {'âœ…' if EMAIL_PASS else 'âŒ'}")
    print(f"   RECEIVER_EMAIL: {'âœ…' if RECEIVER_EMAIL else 'âŒ'}")
    print(f"   ğŸ“§ ì´ë©”ì¼ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
else:
    print(f"âœ… ì´ë©”ì¼ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# Sheet Range Settings
WORKSHEET_RANGE = os.getenv("WORKSHEET_RANGE", "'WORKSHEET'!A1:Z100")
INFO_RANGE = os.getenv("INFO_RANGE", "ì •ë³´íŒ!A1:Z100")

# API í‚¤ë“¤ - í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
REST_API_KEY = os.getenv("KAKAO_REST_API_KEY")
KAKAO_ACCESS_TOKEN = os.getenv("KAKAO_ACCESS_TOKEN")
REFRESH_TOKEN = os.getenv("KAKAO_REFRESH_TOKEN")

if not GITHUB_TOKEN or not REST_API_KEY:
    print("âš ï¸ ì¼ë¶€ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•´ë‹¹ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

if not KAKAO_ACCESS_TOKEN or not REFRESH_TOKEN:
    print(
        "âš ï¸ ì¹´ì¹´ì˜¤í†¡ í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¹´ì¹´ì˜¤í†¡ ì•Œë¦¼ ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )

# ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ ê²½ë¡œ
sheets_json_key_path = os.getenv("SHEETS_KEY_PATH")
drive_json_key_path = os.getenv("DRIVE_KEY_PATH")
spreadsheet_id = os.getenv(
    "SPREADSHEET_ID", "19dkwKNW6VshCg3wTemzmbbQlbATfq6brAWluaps1Rm0"
)
folder_id = DRIVE_FOLDER_ID

SCOPES_SHEETS = ["https://www.googleapis.com/auth/spreadsheets"]
SCOPES_DRIVE = ["https://www.googleapis.com/auth/drive"]

# ì„œë¹„ìŠ¤ ê³„ì • ì¸ì¦ ë° ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)
try:
    if not sheets_json_key_path:
        raise ValueError("SHEETS_KEY_PATH í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if not drive_json_key_path:
        raise ValueError("DRIVE_KEY_PATH í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if not os.path.exists(sheets_json_key_path):
        raise FileNotFoundError(
            f"Sheets ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {sheets_json_key_path}"
        )
    if not os.path.exists(drive_json_key_path):
        raise FileNotFoundError(
            f"Drive ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {drive_json_key_path}"
        )

    sheets_credentials = Credentials.from_service_account_file(
        sheets_json_key_path, scopes=SCOPES_SHEETS
    )
    drive_credentials = Credentials.from_service_account_file(
        drive_json_key_path, scopes=SCOPES_DRIVE
    )

    sheets_service = build("sheets", "v4", credentials=sheets_credentials)
    drive_service = build("drive", "v3", credentials=drive_credentials)

    print("âœ… Google API ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
except Exception as e:
    print(f"âŒ Google API ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    raise

# Font Setting
font_paths = [
    # macOS ê²½ë¡œë“¤
    "/Users/kdkyu311/Library/Fonts/NanumGothic.ttf",
    "/System/Library/AssetsV2/com_apple_MobileAsset_Font7/bad9b4bf17cf1669dde54184ba4431c22dcad27b.asset/AssetData/NanumGothic.ttc",
    "/Library/Fonts/NanumGothic.ttf",
    "/System/Library/Fonts/Supplemental/NanumGothic.ttf",
    # Ubuntu/Linux ê²½ë¡œë“¤ (GitHub Actionsìš©)
    "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
    "/usr/share/fonts/truetype/nanum/NanumGothicBold.ttf",
    "/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf",
    "/usr/share/fonts/opentype/nanum/NanumGothic.ttf",
    # ì¼ë°˜ì ì¸ Linux ì‹œìŠ¤í…œ ê²½ë¡œë“¤
    "/usr/share/fonts/TTF/NanumGothic.ttf",
    "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf",  # ëŒ€ì²´ í°íŠ¸
]
font_path = next((path for path in font_paths if os.path.exists(path)), None)

if font_path:
    print(f"âœ… NanumGothic í°íŠ¸ ì ìš© ì™„ë£Œ: {font_path}")
    font_prop = fm.FontProperties(fname=font_path)
    plt.rc("font", family=font_prop.get_name())
else:
    # ì‹œìŠ¤í…œì—ì„œ ì„¤ì¹˜ëœ í•œê¸€ í°íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ì°¾ê¸°
    print("ğŸ” ì‹œìŠ¤í…œì—ì„œ í•œê¸€ í°íŠ¸ë¥¼ ê²€ìƒ‰ ì¤‘...")
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    korean_fonts = [
        font
        for font in available_fonts
        if any(
            keyword in font.lower()
            for keyword in ["nanum", "malgun", "dotum", "gulim", "batang"]
        )
    ]

    if korean_fonts:
        selected_font = korean_fonts[0]
        print(f"âœ… í•œê¸€ í°íŠ¸ ë°œê²¬ ë° ì ìš©: {selected_font}")
        font_prop = fm.FontProperties(family=selected_font)
        plt.rc("font", family=selected_font)
    else:
        print("ğŸš¨ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        font_prop = None

# ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
plt.rcParams["axes.unicode_minus"] = False


# ë°±ì˜¤í”„ ë°ì½”ë ˆì´í„° ì„¤ì • - 429 (Rate Limit) ì—ëŸ¬ë„ ì¬ì‹œë„í•˜ë„ë¡ ìˆ˜ì •
@on_exception(expo, HttpError, max_tries=10, max_time=300, giveup=lambda e: getattr(e, "response", None) and e.response.status_code not in [429, 503, 500, 502, 504])  # type: ignore
def api_call_with_backoff(func, *args, **kwargs):
    try:
        return func(*args, **kwargs)
    except HttpError as e:
        if getattr(e, "response", None) and e.response.status_code == 429:
            print(f"âš ï¸ [Rate Limit] API í• ë‹¹ëŸ‰ ì´ˆê³¼, ë°±ì˜¤í”„ ì¬ì‹œë„: {e}")
        else:
            print(f"âš ï¸ [Retrying] API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        raise


# --------------------------
# í•¨ìˆ˜: ì‹œíŠ¸ ì´ë¦„ìœ¼ë¡œ sheetId ê°€ì ¸ì˜¤ê¸°
def get_sheet_id_by_name(spreadsheet_id, sheet_name):
    metadata = api_call_with_backoff(
        sheets_service.spreadsheets().get, spreadsheetId=spreadsheet_id
    ).execute()
    for sheet in metadata.get("sheets", []):
        if sheet["properties"]["title"] == sheet_name:
            return sheet["properties"]["sheetId"]
    raise ValueError(f"ì‹œíŠ¸ '{sheet_name}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


# í™˜ê²½ë³€ìˆ˜ì—ì„œ TARGET_SHEET_NAME ì½ê¸°
TARGET_SHEET_NAME = os.getenv("TARGET_SHEET_NAME", "ì¶œí•˜ì˜ˆì •ë¦¬ìŠ¤íŠ¸(TEST)")
print(f"ğŸ“‹ ì‚¬ìš©í•  ì‹œíŠ¸ ì´ë¦„: {TARGET_SHEET_NAME}")
TARGET_SHEET_ID = get_sheet_id_by_name(spreadsheet_id, TARGET_SHEET_NAME)
# --------------------------


# í•¨ìˆ˜: ì „ì²´ ì‹œíŠ¸ ë°ì´í„° í•œ ë²ˆë§Œ ê°€ì ¸ì˜¤ê¸° (TARGET_SHEET_NAME!A:AA)
def fetch_entire_sheet_values(spreadsheet_id, sheet_range=None):
    if sheet_range is None:
        sheet_range = f"'{TARGET_SHEET_NAME}'!A:AA"
    result = api_call_with_backoff(
        sheets_service.spreadsheets().values().get,
        spreadsheetId=spreadsheet_id,
        range=sheet_range,
    ).execute()
    return result.get("values", [])


# HTML & Drive Upload Functions
def generate_html_from_content(html_content, output_filename="index.html"):
    styled_html = f"""
    <html>
    <head>
      <meta charset="UTF-8">
      <title>PDA Dashboard</title>
      <style>
        body {{ font-family: 'NanumGothic', sans-serif; font-size: 12px; margin: 20px; }}
        table {{ border-collapse: collapse; width: 100%; margin-bottom: 20px; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        details {{ margin-bottom: 10px; }}
        summary {{ cursor: pointer; font-weight: bold; color: #333; }}
        details[open] summary {{ color: #0056b3; }}
        details > *:not(summary) {{ display: block; margin-left: 20px; }}
        ul {{ margin: 5px 0; padding-left: 20px; }}
        p {{ margin: 5px 0; }}
        hr {{ margin: 20px 0; }}
        a {{ color: #0056b3; text-decoration: none; }}
        a:hover {{ text-decoration: underline; }}
        img {{ max-width: 100%; height: auto; }}
      </style>
    </head>
    <body>{html_content}</body></html>
    """
    try:
        with open(output_filename, "w", encoding="utf-8") as f:
            f.write(styled_html)
        print(f"ğŸ“„ HTML íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_filename}")
        html_link = upload_to_drive(output_filename)
        if html_link:
            print(f"âœ… HTML ì—…ë¡œë“œ ì™„ë£Œ, ë§í¬: {html_link}")
        return output_filename, html_link
    except Exception as e:
        print(f"âŒ HTML ìƒì„± ì˜¤ë¥˜: {e}")
        return None, None


def upload_to_drive(file_path, drive_service_param=None):
    import os

    from googleapiclient.http import MediaFileUpload

    try:
        # íŒŒì¼ëª…ë§Œ ì¶”ì¶œ (ê²½ë¡œ ì œê±°)
        file_name = os.path.basename(file_path)

        # ì „ì—­ ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        global DRIVE_FOLDER_ID, drive_service

        print(
            f"ğŸ” [DEBUG] Drive ì—…ë¡œë“œ ì‹œë„ - íŒŒì¼: {file_name}, í´ë” ID: {DRIVE_FOLDER_ID}"
        )

        # í´ë” ID ê²€ì¦
        if not DRIVE_FOLDER_ID or DRIVE_FOLDER_ID.strip() == "":
            print(f"âŒ [Drive ì—…ë¡œë“œ ì˜¤ë¥˜] í´ë” IDê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: '{DRIVE_FOLDER_ID}'")
            return None

        file_metadata = {"name": file_name, "parents": [DRIVE_FOLDER_ID]}
        mime_type = (
            "text/html"
            if file_path.endswith(".html")
            else (
                "image/png"
                if file_path.endswith(".png")
                else "application/octet-stream"
            )
        )
        media = MediaFileUpload(file_path, mimetype=mime_type)

        # drive_service íŒŒë¼ë¯¸í„° ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ì „ì—­ ë³€ìˆ˜ ì‚¬ìš©
        service = drive_service_param if drive_service_param else drive_service

        file = api_call_with_backoff(
            service.files().create, body=file_metadata, media_body=media, fields="id"
        ).execute()
        file_id = file.get("id")
        api_call_with_backoff(
            service.permissions().create,
            fileId=file_id,
            body={"type": "anyone", "role": "reader"},
        ).execute()
        image_url = f"https://drive.google.com/uc?export=view&id={file_id}"
        print(f"âœ… Drive ì—…ë¡œë“œ ì™„ë£Œ: {file_name} -> {image_url}")
        return image_url
    except Exception as e:
        print(f"âŒ [Drive ì—…ë¡œë“œ ì˜¤ë¥˜] {file_path}: {e}")
        return None


# Spreadsheet Functions with Batch Processing and reduced read calls
def get_spreadsheet_title(spreadsheet_id):
    try:
        info = api_call_with_backoff(
            sheets_service.spreadsheets().get,
            spreadsheetId=spreadsheet_id,
            fields="properties.title",
        ).execute()
        return info["properties"]["title"]
    except Exception as e:
        print(f"âŒ [ì˜¤ë¥˜] ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì œëª© ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {spreadsheet_id} -> {e}")
        return f"Unknown_{spreadsheet_id}"


def get_order_no(spreadsheet_id):
    return get_spreadsheet_title(spreadsheet_id)


def get_linked_spreadsheet_ids(spreadsheet_id):
    """í•˜ì´í¼ë§í¬ì—ì„œ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ID ì¶”ì¶œ (Rate Limit ë°©ì§€)"""
    import time

    pmmd_hyperlink_range = f"'{TARGET_SHEET_NAME}'!A:A"
    print(f"ğŸ” ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ID ì¶”ì¶œ ì¤‘... (Rate Limit ë°©ì§€ë¥¼ ìœ„í•´ ì²œì²œíˆ ì§„í–‰)")

    # Rate Limit ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
    time.sleep(2)

    result = api_call_with_backoff(
        sheets_service.spreadsheets().values().get,
        spreadsheetId=spreadsheet_id,
        range=pmmd_hyperlink_range,
        valueRenderOption="FORMULA",
    ).execute()

    formulas = result.get("values", [])
    linked_spreadsheet_ids = [
        re.search(r"/d/([a-zA-Z0-9-_]+)", cell).group(1)
        for row in formulas
        for cell in row
        if cell.startswith("=HYPERLINK(")
    ]

    print(
        f"ì¶”ì¶œëœ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ IDë“¤: {linked_spreadsheet_ids[:5]}{'...' if len(linked_spreadsheet_ids) > 5 else ''} (ì´ {len(linked_spreadsheet_ids)}ê°œ)"
    )
    return linked_spreadsheet_ids


# ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ID ì¶”ì¶œ ì‹¤í–‰
linked_spreadsheet_ids = get_linked_spreadsheet_ids(spreadsheet_id)

# Work Schedule Variables
holidays = [
    date(2025, 1, 1),
    date(2025, 1, 27),
    date(2025, 1, 28),
    date(2025, 1, 29),
    date(2025, 3, 1),
    date(2025, 5, 5),
    date(2025, 5, 6),
    date(2025, 6, 6),
    date(2025, 8, 15),
    date(2025, 10, 3),
    date(2025, 10, 6),
    date(2025, 10, 7),
    date(2025, 10, 8),
    date(2025, 10, 9),
    date(2025, 12, 25),
]
WORK_START, WORK_END, MAX_DAILY_HOURS = time(8, 0, 0), time(20, 0, 0), 12
LUNCH_START, LUNCH_END = time(11, 20, 0), time(12, 20, 0)
DINNER_START, DINNER_END = time(17, 0, 0), time(18, 0, 0)
BREAK_1_START, BREAK_1_END = time(10, 0, 0), time(10, 20, 0)
BREAK_2_START, BREAK_2_END = time(15, 0, 0), time(15, 20, 0)


# ====================================
# Utility Functions
# ====================================
def parse_korean_datetime(dt_input):
    if isinstance(dt_input, (int, float)):
        try:
            return pd.to_datetime(dt_input, unit="D", origin="1899-12-30")
        except:
            return pd.NaT
    elif isinstance(dt_input, str):
        s = dt_input.strip()
        if not s:
            return pd.NaT
        if re.match(r"^\d{4}\.\s*\d{1,2}\.\s*\d{1,2}\s*$", s):
            s += " 00:00:00"
        s = s.replace("ì˜¤ì „", "AM").replace("ì˜¤í›„", "PM")
        fmt = (
            "%Y. %m. %d %p %I:%M:%S"
            if "AM" in s or "PM" in s
            else "%Y. %m. %d %H:%M:%S"
        )
        return pd.to_datetime(s, format=fmt, errors="coerce")
    return pd.NaT


# ì¤‘ë³µ í•¨ìˆ˜ ì œê±°ë¨ - ì•„ë˜ì˜ ê°œì„ ëœ ë²„ì „ ì‚¬ìš©


def calculate_working_hours_with_holidays(start_time, end_time):
    if pd.isna(start_time) or pd.isna(end_time):
        return 0
    total_hours = 0
    current_time = start_time
    while current_time < end_time:
        day_of_week = current_time.weekday()
        work_start = datetime.combine(
            current_time.date(), time(8, 0, 0) if day_of_week in [5, 6] else WORK_START
        )
        work_end = datetime.combine(
            current_time.date(), time(17, 0, 0) if day_of_week in [5, 6] else WORK_END
        )
        work_start = max(work_start, current_time)
        work_end = min(work_end, end_time)
        daily_hours = (work_end - work_start).total_seconds() / 3600
        breaks = [
            (LUNCH_START, LUNCH_END),
            (BREAK_1_START, BREAK_1_END),
            (BREAK_2_START, BREAK_2_END),
            (DINNER_START, DINNER_END),
        ]
        for b_start, b_end in breaks:
            b_start_dt = datetime.combine(current_time.date(), b_start)
            b_end_dt = datetime.combine(current_time.date(), b_end)
            if work_start < b_end_dt and b_start_dt < work_end:
                daily_hours -= (
                    min(work_end, b_end_dt) - max(work_start, b_start_dt)
                ).total_seconds() / 3600
        total_hours += min(daily_hours, 9 if day_of_week in [5, 6] else MAX_DAILY_HOURS)
        current_time = datetime.combine(
            current_time.date() + timedelta(days=1), WORK_START
        )
    return total_hours


def process_data(df_use, model_name):
    df_complete = df_use.dropna(subset=["ì‹œì‘ ì‹œê°„", "ì™„ë£Œ ì‹œê°„"]).copy()
    df_complete["ì›Œí‚¹ë°ì´ ì†Œìš” ì‹œê°„"] = df_complete.apply(
        lambda row: calculate_working_hours_with_holidays(
            row["ì‹œì‘ ì‹œê°„"], row["ì™„ë£Œ ì‹œê°„"]
        ),
        axis=1,
    )
    df_complete["ì‘ì—… ë¶„ë¥˜"] = df_complete["ë‚´ìš©"].apply(
        lambda x: classify_task(x, model_name)
    )
    task_total_time = (
        df_complete.groupby("ë‚´ìš©")["ì›Œí‚¹ë°ì´ ì†Œìš” ì‹œê°„"].sum().reset_index()
    )
    task_total_time["ì´ ì›Œí‚¹ ì†Œìš” ì‹œê°„ (ì‹œê°„:ë¶„)"] = task_total_time[
        "ì›Œí‚¹ë°ì´ ì†Œìš” ì‹œê°„"
    ].apply(format_hours)
    return task_total_time.sort_values("ì›Œí‚¹ë°ì´ ì†Œìš” ì‹œê°„", ascending=True)


def format_hours(decimal_hours):
    hours = int(decimal_hours)
    minutes = int(round((decimal_hours - hours) * 60))
    return f"{hours}h {minutes}m"


# ====================================
# Task Classification
# ====================================
default_mechanical_tasks = [
    "CABINET ASSY",
    "BURNER ASSY(TMS)",
    "WET TANK ASSY(TMS)",
    "3-WAY VALVE ASSY",
    "N2 LINE ASSY",
    "N2 TUBE ASSY",
    "CDA LINE ASSY",
    "CDA TUBE ASSY",
    "BCW LINE ASSY",
    "PCW LINE ASSY",
    "O2 LINE ASSY",
    "LNG LINE ASSY",
    "WASTE GAS LINE ASSY",
    "COOLING UNIT(TMS)",
    "REACTOR ASSY(TMS)",
    "HEATING JACKET",
    "CIR LINE TUBING",
    "ì„¤ë¹„ CLEANING",
    "ìì£¼ê²€ì‚¬",
]
default_electrical_tasks = [
    "AC ë°± íŒë„¬ ì‘ì—…",
    "DC ë°± íŒë„¬ ì‘ì—…",
    "ì¼€ë¹„ë„· ì¤€ë¹„ ì‘ì—…(ë•íŠ¸, ì² ê±°ì‘ì—…)",
    "íŒë„¬ ì·¨ë¶€ ë° ì„ ë¶„ë¦¬",
    "ë‚´, ì™¸ë¶€ ì‘ì—…",
    "íƒ±í¬ ì‘ì—…",
    "íŒë„¬ ì‘ì—…",
    "íƒ±í¬ ë„í‚¹ í›„ ê²°ì„  ì‘ì—…",
]
default_inspection_tasks = ["LNG/Util", "Chamber", "I/O ì²´í¬, ê°€ë™ ê²€ì‚¬, ì „ì¥ ë§ˆë¬´ë¦¬"]
default_finishing_tasks = ["ìºë¹„ë„· ì»¤ë²„ ì¥ì°© ë° í¬ì¥", "ìƒë¶€ ë§ˆë¬´ë¦¬"]

model_mechanical_tasks = {
    "GAIA-I DUAL": default_mechanical_tasks,
    "GAIA-I": default_mechanical_tasks,
    "DRAGON": default_mechanical_tasks,
    "DRAGON DUAL": default_mechanical_tasks,
    "GAIA-II": default_mechanical_tasks,
    "SWS-I": [
        "CABINET ASSY",
        "BURNER ASSY(TMS)",
        "WET TANK ASSY(TMS)",
        "3-WAY VALVE ASSY",
        "N2 LINE ASSY",
        "N2 TUBE ASSY",
        "BCW LINE ASSY",
        "WASTE GAS LINE ASSY",
        "COOLING UNIT(TMS)",
        "REACTOR ASSY(TMS)",
        "HEATING JACKET",
        "CIR LINE TUBING",
        "ì„¤ë¹„ CLEANING",
        "ìì£¼ê²€ì‚¬",
    ],
    "GAIA-P DUAL": [
        "CABINET ASSY",
        "BURNER ASSY(TMS)",
        "WET TANK ASSY(TMS)",
        "3-WAY VALVE ASSY",
        "N2 LINE ASSY",
        "N2 TUBE ASSY",
        "CDA LINE ASSY",
        "CDA TUBE ASSY",
        "BCW LINE ASSY",
        "PCW LINE ASSY",
        "WASTE GAS LINE ASSY",
        "COOLING UNIT(TMS)",
        "REACTOR ASSY(TMS)",
        "HEATING JACKET",
        "CIR LINE TUBING",
        "ì„¤ë¹„ CLEANING",
        "ìì£¼ê²€ì‚¬",
    ],
    "GAIA-P": [
        "CABINET ASSY",
        "BURNER ASSY(TMS)",
        "WET TANK ASSY(TMS)",
        "3-WAY VALVE ASSY",
        "N2 LINE ASSY",
        "N2 TUBE ASSY",
        "CDA LINE ASSY",
        "CDA TUBE ASSY",
        "BCW LINE ASSY",
        "PCW LINE ASSY",
        "WASTE GAS LINE ASSY",
        "COOLING UNIT(TMS)",
        "REACTOR ASSY(TMS)",
        "HEATING JACKET",
        "CIR LINE TUBING",
        "ì„¤ë¹„ CLEANING",
        "ìì£¼ê²€ì‚¬",
    ],
    "IVAS": [
        "CABINET ASSY",
        "BURNER ASSY(TMS)",
        "WET TANK ASSY(TMS)",
        "3-WAY VALVE ASSY",
        "N2 LINE ASSY",
        "N2 TUBE ASSY",
        "CDA LINE ASSY",
        "CDA TUBE ASSY",
        "BCW LINE ASSY",
        "PCW LINE ASSY",
        "O2 LINE ASSY",
        "LNG LINE ASSY",
        "WASTE GAS LINE ASSY",
        "COOLING UNIT(TMS)",
        "REACTOR ASSY(TMS)",
        "HEATING JACKET",
        "CIR LINE TUBING",
        "ì„¤ë¹„ CLEANING",
        "ìì£¼ê²€ì‚¬",
    ],
}


def get_mechanical_tasks(model_name):
    return model_mechanical_tasks.get(model_name.upper(), default_mechanical_tasks)


def classify_task(content, model_name):
    model_name = model_name.upper()
    tms_tasks = [
        "BURNER ASSY(TMS)",
        "WET TANK ASSY(TMS)",
        "COOLING UNIT(TMS)",
        "REACTOR ASSY(TMS)",
    ]

    # DRAGON, DRAGON DUAL, SWS-I ëª¨ë¸ì—ì„œëŠ” tms_tasksë„ ê¸°êµ¬ë¡œ ë¶„ë¥˜
    if model_name in [
        "DRAGON",
        "DRAGON DUAL",
        "SWS-I",
    ] and content in get_mechanical_tasks(model_name):
        return "ê¸°êµ¬"

    if content in tms_tasks and model_name not in ["DRAGON", "DRAGON DUAL", "SWS-I"]:
        return "TMS_ë°˜ì œí’ˆ"
    elif content in get_mechanical_tasks(model_name):
        return "ê¸°êµ¬"
    elif content in default_electrical_tasks:
        return "ì „ì¥"
    elif content in default_inspection_tasks:
        return "ê²€ì‚¬"
    elif content in default_finishing_tasks:
        return "ë§ˆë¬´ë¦¬"
    return "ê¸°íƒ€"


def calculate_progress_by_category(df, model_name):
    df = df.copy()
    df["ì‘ì—… ë¶„ë¥˜"] = df["ë‚´ìš©"].apply(lambda x: classify_task(x, model_name))
    df["ì§„í–‰ìœ¨"] = df.apply(
        lambda row: (
            100.0
            if pd.isna(row["ì§„í–‰ìœ¨"])
            and pd.notna(row["ì‹œì‘ ì‹œê°„"])
            and pd.notna(row["ì™„ë£Œ ì‹œê°„"])
            else row["ì§„í–‰ìœ¨"]
        ),
        axis=1,
    )
    df_valid = df.dropna(subset=["ë‚´ìš©"])
    df_max = df_valid.groupby(["ë‚´ìš©", "ì‘ì—… ë¶„ë¥˜"])["ì§„í–‰ìœ¨"].max().reset_index()
    progress_summary = {}
    for category in ["ê¸°êµ¬", "ì „ì¥", "TMS_ë°˜ì œí’ˆ"]:
        df_cat = df_max[df_max["ì‘ì—… ë¶„ë¥˜"] == category]
        total_tasks = len(df_cat)
        completed = df_cat[df_cat["ì§„í–‰ìœ¨"] == 100.0]
        progress = (len(completed) / total_tasks * 100) if total_tasks > 0 else 0
        progress_summary[category] = round(progress, 1)
    return progress_summary


def parse_avg_time_string(s):
    s = s.lower().strip()
    match = re.match(r"(?:(\d+)\s*h)?\s*(?:(\d+)\s*m)?", s)
    hours = int(match.group(1)) if match and match.group(1) else 0
    minutes = int(match.group(2)) if match and match.group(2) else 0
    return hours + minutes / 60.0


def get_avg_time_mapping(model_name):
    avg_spreadsheet_id = "1PHKsQ-3kcyaB9HdJqdaLN4siqnHRE8FC7XzGR2pmoLc"
    sheet_range = f"'{model_name.strip()}'!A:B"
    try:
        avg_values = (
            sheets_service.spreadsheets()
            .values()
            .get(
                spreadsheetId=avg_spreadsheet_id,
                range=sheet_range,
                valueRenderOption="FORMATTED_VALUE",
            )
            .execute()
            .get("values", [])
        )
        if len(avg_values) <= 1:
            return {}
        return {
            row[0].strip(): (
                parse_avg_time_string(row[1])
                if "h" in row[1].lower() or "m" in row[1].lower()
                else float(row[1])
            )
            for row in avg_values[1:]
            if len(row) >= 2
        }
    except Exception as e:
        print(f"[ì˜¤ë¥˜] AVDATAì—ì„œ '{model_name}' ì‹œíŠ¸ë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {}


# ====================================
# Graph Functions
# ====================================
def generate_and_save_graph(task_total_time, order_no, model_name):
    avg_mapping = get_avg_time_mapping(model_name)
    fig, ax = plt.subplots(figsize=(12, 8))
    bars = ax.barh(
        task_total_time["ë‚´ìš©"], task_total_time["ì›Œí‚¹ë°ì´ ì†Œìš” ì‹œê°„"], color="skyblue"
    )
    ax.set_yticks(range(len(task_total_time)))
    ax.set_yticklabels(
        [
            (
                f"{task} (í‰ê· : {format_hours(avg_mapping[task])})"
                if task in avg_mapping
                else task
            )
            for task in task_total_time["ë‚´ìš©"]
        ],
        fontsize=10,
        fontweight="bold",
        color="blue",
    )
    for bar in bars:
        width = bar.get_width()
        ax.text(
            width + 0.2,
            bar.get_y() + bar.get_height() / 2,
            f"{int(width)}h {int(round((width - int(width)) * 60))}m",
            va="center",
            ha="left",
            color="black",
        )
    ax.set_xlim(0, max(bar.get_width() for bar in bars) + 1)
    ax.set_xlabel("Working Hours")
    ax.set_title(f"Total Working Hours per Task for {order_no}")
    plt.tight_layout()
    file_name = f"Total_Working_Hours_{order_no.replace('/', '_')}_{model_name}.png"
    plt.savefig(file_name)
    plt.close()
    return file_name


def generate_legend_chart(task_total_time, order_no, model_name):
    avg_mapping = get_avg_time_mapping(model_name)
    task_total_time["ì‘ì—… ë¶„ë¥˜"] = task_total_time["ë‚´ìš©"].apply(
        lambda x: classify_task(x, model_name)
    )
    task_total_time_sorted = task_total_time.sort_values(
        by=["ì‘ì—… ë¶„ë¥˜", "ì›Œí‚¹ë°ì´ ì†Œìš” ì‹œê°„"], ascending=[True, False]
    )
    category_totals = task_total_time_sorted.groupby("ì‘ì—… ë¶„ë¥˜")[
        "ì›Œí‚¹ë°ì´ ì†Œìš” ì‹œê°„"
    ].sum()
    total_time = category_totals.sum()
    category_colors = {
        "ê¸°êµ¬": "blue",
        "TMS_ë°˜ì œí’ˆ": "cyan",
        "ì „ì¥": "orange",
        "ê²€ì‚¬": "green",
        "ë§ˆë¬´ë¦¬": "red",
        "ê¸°íƒ€": "gray",
    }
    legend_elements = [
        Patch(
            facecolor="black",
            edgecolor="black",
            label=f"ì´ ì†Œìš”ì‹œê°„: {format_hours(total_time)}",
        )
    ]
    for category, color in category_colors.items():
        if category in category_totals:
            category_time = category_totals[category]
            legend_elements.append(
                Patch(
                    facecolor=color,
                    edgecolor="black",
                    label=f"{category} (ì´ {format_hours(category_time)})",
                )
            )
            for _, row in task_total_time_sorted[
                task_total_time_sorted["ì‘ì—… ë¶„ë¥˜"] == category
            ].iterrows():
                avg_str = (
                    f" (í‰ê· : {format_hours(avg_mapping[row['ë‚´ìš©']])})"
                    if row["ë‚´ìš©"] in avg_mapping
                    else ""
                )
                legend_elements.append(
                    Patch(
                        facecolor="white",
                        edgecolor=color,
                        label=f"  {row['ë‚´ìš©']}: {row['ì´ ì›Œí‚¹ ì†Œìš” ì‹œê°„ (ì‹œê°„:ë¶„)']}{avg_str}",
                    )
                )
    plt.figure(figsize=(8, len(legend_elements) * 0.3))
    legend = plt.legend(
        handles=legend_elements,
        loc="center",
        fontsize=10,
        title="ì‘ì—… ë¶„ë¥˜ ë° ì‘ì—…ë³„ ì†Œìš” ì‹œê°„ (ë‚´ë¦¼ì°¨ìˆœ)",
        frameon=False,
    )
    plt.axis("off")
    plt.title(f"{order_no}", fontsize=12, loc="center", color="black")
    if legend.get_texts():
        legend.get_texts()[0].set_color("red")
    file_name = f"Legend_Chart_{order_no.replace('/', '_')}_{model_name}.png"
    plt.savefig(file_name)
    plt.close()
    return file_name


def generate_and_save_graph_wd(task_total_time, df, order_no, model_name):
    df_valid = df.dropna(subset=["ì‹œì‘ ì‹œê°„", "ì™„ë£Œ ì‹œê°„"])
    plt.figure(figsize=(16, 10))
    colors = plt.cm.tab20.colors
    time_offset = pd.Timedelta(hours=1)
    for index, task in enumerate(task_total_time["ë‚´ìš©"]):
        group = df_valid[df_valid["ë‚´ìš©"] == task].sort_values("ì‹œì‘ ì‹œê°„")
        if group.empty:
            continue
        total_duration_text = task_total_time.loc[
            task_total_time["ë‚´ìš©"] == task, "ì´ ì›Œí‚¹ ì†Œìš” ì‹œê°„ (ì‹œê°„:ë¶„)"
        ].values[0]
        for i, (_, row) in enumerate(group.iterrows()):
            start_time = row["ì‹œì‘ ì‹œê°„"] + (i * time_offset)
            end_time = row["ì™„ë£Œ ì‹œê°„"] + (i * time_offset)
            plt.plot(
                [start_time, end_time],
                [index, index],
                color=colors[index % len(colors)],
                linewidth=3,
                marker="o",
            )
        plt.text(
            group["ì™„ë£Œ ì‹œê°„"].max() + pd.Timedelta(hours=2),
            index,
            total_duration_text,
            va="center",
            fontsize=10,
            color="black",
        )
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter("%m-%d"))
    plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=1))
    plt.xticks(rotation=45)
    plt.yticks(range(len(task_total_time["ë‚´ìš©"])), task_total_time["ë‚´ìš©"])
    plt.xlabel("Date")
    plt.ylabel("Tasks")
    plt.title(f"Task Time Chart with Total WD Duration - {order_no}")
    plt.grid(axis="x", linestyle="--", alpha=0.7)
    plt.tight_layout()
    file_name = f"WD_Working_Hours_{order_no.replace('/', '_')}_{model_name}.png"
    plt.savefig(file_name)
    plt.close()
    return file_name


# Utility Functions
def fetch_data_from_sheets(spreadsheet_id, sheet_range):
    result = api_call_with_backoff(
        sheets_service.spreadsheets().values().get,
        spreadsheetId=spreadsheet_id,
        range=sheet_range,
    ).execute()
    values = result.get("values", [])
    if not values or len(values) <= 7:
        raise ValueError(f"'{sheet_range}'ì— ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    header = values[6]
    data = values[7:]
    max_cols = max(len(header), max((len(row) for row in data), default=0))
    if max_cols > len(header):
        header.extend([""] * (max_cols - len(header)))
    adjusted_data = [
        row + [""] * (max_cols - len(row)) if len(row) < max_cols else row[:max_cols]
        for row in data
    ]
    df_raw = pd.DataFrame(adjusted_data, columns=header[:max_cols])
    needed_cols = ["ë‚´ìš©", "ì‹œì‘ ì‹œê°„", "ì™„ë£Œ ì‹œê°„", "ì§„í–‰ìœ¨"]
    if not all(col in df_raw.columns for col in needed_cols):
        raise ValueError(f"í•„ìš”í•œ ì»¬ëŸ¼ {needed_cols}ì´(ê°€) '{sheet_range}'ì— ì—†ìŠµë‹ˆë‹¤.")
    df_use = df_raw[needed_cols].copy()
    df_use["ì‹œì‘ ì‹œê°„"] = df_use["ì‹œì‘ ì‹œê°„"].apply(parse_korean_datetime)
    df_use["ì™„ë£Œ ì‹œê°„"] = df_use["ì™„ë£Œ ì‹œê°„"].apply(parse_korean_datetime)
    # ì§„í–‰ìœ¨ float ë³€í™˜ì— ë°©ì–´ì½”ë“œ ë° ë¡œê¹… ì¶”ê°€
    try:
        df_use["ì§„í–‰ìœ¨"] = (
            df_use["ì§„í–‰ìœ¨"]
            .astype(str)
            .str.replace("%", "")
            .str.strip()
            .replace("", np.nan)
            .astype(float)
        )
    except Exception as e:
        print(f"[ì§„í–‰ìœ¨ ë³€í™˜ ì˜¤ë¥˜] {e}")
        print("ì§„í–‰ìœ¨ ê°’ ëª©ë¡:", df_use["ì§„í–‰ìœ¨"].unique())
        raise
    return df_use[df_use["ë‚´ìš©"].notna()]


def fetch_info_board_extended(spreadsheet_id):
    ranges = [
        ("ì •ë³´íŒ!D4", "model_name"),
        ("ì •ë³´íŒ!B5", "mech_partner"),
        ("ì •ë³´íŒ!D5", "elec_partner"),
    ]
    batch_request = (
        sheets_service.spreadsheets()
        .values()
        .batchGet(
            spreadsheetId=spreadsheet_id,
            ranges=[rng for rng, _ in ranges],
            valueRenderOption="FORMATTED_VALUE",
        )
    )
    result = api_call_with_backoff(batch_request.execute)
    results = {}
    for (rng, key), response in zip(ranges, result.get("valueRanges", [])):
        values = response.get("values", [[]])
        results[key] = values[0][0].strip() if values else "ë¯¸ì •"
    print(
        f"ğŸ“Œ [ë””ë²„ê¹…] ëª¨ë¸ëª…: {results['model_name']}, ê¸°êµ¬í˜‘ë ¥ì‚¬: {results['mech_partner']}, ì „ì¥í˜‘ë ¥ì‚¬: {results['elec_partner']}"
    )
    return (
        results["model_name"] or "NoValue",
        results["mech_partner"],
        results["elec_partner"],
    )


def batch_update_spreadsheet(spreadsheet_id, requests):
    body = {"requests": requests}
    api_call_with_backoff(
        sheets_service.spreadsheets().batchUpdate,
        spreadsheetId=spreadsheet_id,
        body=body,
    ).execute()


# --- ìˆ˜ì •ëœ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ (sheet_values ì „ë‹¬) ---
def update_spreadsheet_with_product_name(
    spreadsheet_id, order_no, product_name, sheet_values
):
    if not product_name or product_name == "NoValue":
        print(f"âš ï¸ ì œí’ˆëª…ì´ ë¹„ì–´ ìˆìŒ (Row: {order_no})")
        return
    if not sheet_values:
        print("ğŸš¨ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    requests = []
    for i, row in enumerate(sheet_values[1:], 2):
        if row and row[0].strip().lower() == order_no.strip().lower():
            requests.append(
                {
                    "updateCells": {
                        "range": {
                            "sheetId": TARGET_SHEET_ID,
                            "startRowIndex": i - 1,
                            "endRowIndex": i,
                            "startColumnIndex": 3,
                            "endColumnIndex": 4,
                        },
                        "rows": [
                            {
                                "values": [
                                    {"userEnteredValue": {"stringValue": product_name}}
                                ]
                            }
                        ],
                        "fields": "userEnteredValue",
                    }
                }
            )
    if requests:
        batch_update_spreadsheet(spreadsheet_id, requests)
        print(
            f"âœ… {TARGET_SHEET_NAME}ì—ì„œ Order No '{order_no}'ì˜ ì œí’ˆëª…ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤."
        )


def update_spreadsheet_with_total_time(
    spreadsheet_id, order_no, total_time, sheet_values
):
    if not sheet_values:
        print("ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    requests = []
    for i, row in enumerate(sheet_values[1:], 2):
        if row and row[0].strip().lower() == order_no.strip().lower():
            requests.append(
                (
                    {
                        "sheetId": TARGET_SHEET_ID,
                        "startRowIndex": i - 1,
                        "endRowIndex": i,
                        "startColumnIndex": 22,
                        "endColumnIndex": 23,
                    },
                    total_time,
                )
            )
    batch_update_spreadsheet_values(spreadsheet_id, requests)
    print(f"ëª¨ë¸ '{order_no}'ì˜ ì´ ì†Œìš”ì‹œê°„ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")


def update_spreadsheet_with_mechanical_time(
    spreadsheet_id, order_no, mechanical_time, sheet_values
):
    if not sheet_values:
        print("ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    requests = []
    for i, row in enumerate(sheet_values[1:], 2):
        if row and row[0].strip().lower() == order_no.strip().lower():
            requests.append(
                (
                    {
                        "sheetId": TARGET_SHEET_ID,
                        "startRowIndex": i - 1,
                        "endRowIndex": i,
                        "startColumnIndex": 23,
                        "endColumnIndex": 24,
                    },
                    mechanical_time,
                )
            )
    batch_update_spreadsheet_values(spreadsheet_id, requests)
    print(f"ëª¨ë¸ '{order_no}'ì˜ ê¸°êµ¬ì‘ì—… ì†Œìš”ì‹œê°„ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")


def update_spreadsheet_with_electrical_time(
    spreadsheet_id, order_no, electrical_time, sheet_values
):
    if not sheet_values:
        print("ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    requests = []
    for i, row in enumerate(sheet_values[1:], 2):
        if row and row[0].strip().lower() == order_no.strip().lower():
            requests.append(
                (
                    {
                        "sheetId": TARGET_SHEET_ID,
                        "startRowIndex": i - 1,
                        "endRowIndex": i,
                        "startColumnIndex": 24,
                        "endColumnIndex": 25,
                    },
                    electrical_time,
                )
            )
    batch_update_spreadsheet_values(spreadsheet_id, requests)
    print(f"ëª¨ë¸ '{order_no}'ì˜ ì „ì¥ ì‘ì—… ì‹œê°„ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")


def update_spreadsheet_with_inspection_time(
    spreadsheet_id, order_no, inspection_time, sheet_values
):
    if not sheet_values:
        print("ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    requests = []
    for i, row in enumerate(sheet_values[1:], 2):
        if row and row[0].strip().lower() == order_no.strip().lower():
            requests.append(
                (
                    {
                        "sheetId": TARGET_SHEET_ID,
                        "startRowIndex": i - 1,
                        "endRowIndex": i,
                        "startColumnIndex": 25,
                        "endColumnIndex": 26,
                    },
                    inspection_time,
                )
            )
    batch_update_spreadsheet_values(spreadsheet_id, requests)
    print(f"ëª¨ë¸ '{order_no}'ì˜ ê²€ì‚¬ ì‘ì—… ì‹œê°„ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")


def update_spreadsheet_with_finishing_time(
    spreadsheet_id, order_no, finishing_time, sheet_values
):
    if not sheet_values:
        print("ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    requests = []
    for i, row in enumerate(sheet_values[1:], 2):
        if row and row[0].strip().lower() == order_no.strip().lower():
            requests.append(
                (
                    {
                        "sheetId": TARGET_SHEET_ID,
                        "startRowIndex": i - 1,
                        "endRowIndex": i,
                        "startColumnIndex": 26,
                        "endColumnIndex": 27,
                    },
                    finishing_time,
                )
            )
    batch_update_spreadsheet_values(spreadsheet_id, requests)
    print(f"ëª¨ë¸ '{order_no}'ì˜ ë§ˆë¬´ë¦¬ ì‘ì—… ì‹œê°„ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")


def update_spreadsheet_with_working_hours(spreadsheet_id, order_no, link, sheet_values):
    if not sheet_values:
        print("ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return link
    requests = []
    for i, row in enumerate(sheet_values[1:], 2):
        if row and row[0].strip().lower() == order_no.strip().lower():
            # í•˜ì´í¼ë§í¬ ê³µì‹ì„ ì§ì ‘ ì…ë ¥ (ì•ì˜ ' ë°©ì§€)
            requests.append(
                {
                    "updateCells": {
                        "range": {
                            "sheetId": TARGET_SHEET_ID,
                            "startRowIndex": i - 1,
                            "endRowIndex": i,
                            "startColumnIndex": 21,
                            "endColumnIndex": 22,
                        },
                        "rows": [
                            {
                                "values": [
                                    {
                                        "userEnteredValue": {
                                            "formulaValue": f'=HYPERLINK("{link}", "Working Hours")'
                                        }
                                    }
                                ]
                            }
                        ],
                        "fields": "userEnteredValue",
                    }
                }
            )
    if requests:
        batch_update_spreadsheet(spreadsheet_id, requests)
    print(f"ëª¨ë¸ '{order_no}'ì˜ WORKING HOURS ê·¸ë˜í”„ ë§í¬ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return link


def update_spreadsheet_with_legend(spreadsheet_id, order_no, link, sheet_values):
    if not sheet_values:
        print("ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return link
    requests = []
    for i, row in enumerate(sheet_values[1:], 2):
        if row and row[0].strip().lower() == order_no.strip().lower():
            # í•˜ì´í¼ë§í¬ ê³µì‹ì„ ì§ì ‘ ì…ë ¥ (ì•ì˜ ' ë°©ì§€)
            requests.append(
                {
                    "updateCells": {
                        "range": {
                            "sheetId": TARGET_SHEET_ID,
                            "startRowIndex": i - 1,
                            "endRowIndex": i,
                            "startColumnIndex": 20,
                            "endColumnIndex": 21,
                        },
                        "rows": [
                            {
                                "values": [
                                    {
                                        "userEnteredValue": {
                                            "formulaValue": f'=HYPERLINK("{link}", "Legend Chart")'
                                        }
                                    }
                                ]
                            }
                        ],
                        "fields": "userEnteredValue",
                    }
                }
            )
    if requests:
        batch_update_spreadsheet(spreadsheet_id, requests)
    print(f"ëª¨ë¸ '{order_no}'ì˜ ë²”ë¡€ ì°¨íŠ¸ ë§í¬ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return link


def update_spreadsheet_with_wd_graph(spreadsheet_id, order_no, link, sheet_values):
    if not sheet_values:
        print("ğŸš¨ [ì˜¤ë¥˜] ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return link
    requests = []
    for i, row in enumerate(sheet_values[1:], 2):
        if row and row[0].strip().lower() == order_no.strip().lower():
            # í•˜ì´í¼ë§í¬ ê³µì‹ì„ ì§ì ‘ ì…ë ¥ (ì•ì˜ ' ë°©ì§€)
            requests.append(
                {
                    "updateCells": {
                        "range": {
                            "sheetId": TARGET_SHEET_ID,
                            "startRowIndex": i - 1,
                            "endRowIndex": i,
                            "startColumnIndex": 28,
                            "endColumnIndex": 29,
                        },
                        "rows": [
                            {
                                "values": [
                                    {
                                        "userEnteredValue": {
                                            "formulaValue": f'=HYPERLINK("{link}", "WD Chart")'
                                        }
                                    }
                                ]
                            }
                        ],
                        "fields": "userEnteredValue",
                    }
                }
            )
    if requests:
        batch_update_spreadsheet(spreadsheet_id, requests)
    print(f"ëª¨ë¸ '{order_no}'ì˜ WD ì‘ì—…ì‹œê°„ ê·¸ë˜í”„ ë§í¬ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return link


# Helper: Batch update for values (used by update functions)
def batch_update_spreadsheet_values(spreadsheet_id, data):
    requests = []
    for range_spec, value in data:
        requests.append(
            {
                "updateCells": {
                    "range": range_spec,
                    "rows": [
                        {"values": [{"userEnteredValue": {"stringValue": str(value)}}]}
                    ],
                    "fields": "userEnteredValue",
                }
            }
        )
    if requests:
        batch_update_spreadsheet(spreadsheet_id, requests)


# NaN & Overtime Stats
def compute_occurrence_rates(
    df,
    task_total_time,
    avg_mapping,
    model_name,
    tolerance=2,
    mech_partner=None,
    elec_partner=None,
):
    categories = ["ê¸°êµ¬", "TMS_ë°˜ì œí’ˆ", "ì „ì¥", "ê²€ì‚¬", "ë§ˆë¬´ë¦¬", "ê¸°íƒ€"]
    occurrence_stats = {
        cat: {
            "total_count": 0,
            "nan_count": 0,
            "ot_count": 0,
            "nan_tasks": [],
            "ot_task_details": [],
        }
        for cat in categories
    }
    partner_stats = {
        "mech": {"nan_count": 0, "ot_count": 0},
        "elec": {"nan_count": 0, "ot_count": 0},
    }
    df["ì§„í–‰ìœ¨"] = pd.to_numeric(df["ì§„í–‰ìœ¨"], errors="coerce")
    completed_tasks = set(
        df[
            (pd.to_numeric(df["ì§„í–‰ìœ¨"], errors="coerce") >= 100)
            | (df["ì‹œì‘ ì‹œê°„"].notna() & df["ì™„ë£Œ ì‹œê°„"].notna())
        ]["ë‚´ìš©"]
    )
    nan_task_checked = set()
    for _, row in df.iterrows():
        task_name = row["ë‚´ìš©"]
        category = classify_task(task_name, model_name)
        occurrence_stats[category]["total_count"] += 1
        is_nan = (
            pd.isna(row["ì‹œì‘ ì‹œê°„"])
            or pd.isna(row["ì™„ë£Œ ì‹œê°„"])
            or pd.isna(row["ì§„í–‰ìœ¨"])
        )
        if is_nan:
            if task_name in completed_tasks:
                continue
            if (task_name, category) in nan_task_checked:
                continue
            occurrence_stats[category]["nan_count"] += 1
            occurrence_stats[category]["nan_tasks"].append(task_name)
            nan_task_checked.add((task_name, category))
            if category == "ê¸°êµ¬":
                partner_stats["mech"]["nan_count"] += 1
            elif category == "ì „ì¥":
                partner_stats["elec"]["nan_count"] += 1
    for _, row in task_total_time.iterrows():
        task_name = row["ë‚´ìš©"]
        actual_hours = row["ì›Œí‚¹ë°ì´ ì†Œìš” ì‹œê°„"]
        category = classify_task(task_name, model_name)
        if (
            task_name in avg_mapping
            and actual_hours > avg_mapping[task_name] + tolerance
        ):
            occurrence_stats[category]["ot_count"] += 1
            occurrence_stats[category]["ot_task_details"].append(
                (task_name, actual_hours)
            )
            if category == "ê¸°êµ¬":
                partner_stats["mech"]["ot_count"] += 1
            elif category == "ì „ì¥":
                partner_stats["elec"]["ot_count"] += 1
    return occurrence_stats, partner_stats


# Bar Chart Generation
def generate_nan_bar_charts(all_results):
    partner_stats = {}
    for (
        _,
        _,
        mech_partner_raw,
        elec_partner_raw,
        occurrence_stats,
        partner_stats_individual,
        _,
        _,
        _,
    ) in all_results:
        mech_partner = "TMS(m)" if mech_partner_raw == "TMS" else mech_partner_raw
        elec_partner = "TMS(e)" if elec_partner_raw == "TMS" else elec_partner_raw
        mech_nan = partner_stats_individual.get("mech", {}).get("nan_count", 0)
        mech_total = occurrence_stats.get("ê¸°êµ¬", {}).get("total_count", 0)
        if mech_partner:
            partner_stats[mech_partner] = partner_stats.get(
                mech_partner, {"nan_count": 0, "total_tasks": 0}
            )
            partner_stats[mech_partner]["nan_count"] += mech_nan
            partner_stats[mech_partner]["total_tasks"] += mech_total
        tms_nan = occurrence_stats.get("TMS_ë°˜ì œí’ˆ", {}).get("nan_count", 0)
        tms_total = occurrence_stats.get("TMS_ë°˜ì œí’ˆ", {}).get("total_count", 0)
        if tms_total > 0:
            partner_stats["TMS_ë°˜ì œí’ˆ"] = partner_stats.get(
                "TMS_ë°˜ì œí’ˆ", {"nan_count": 0, "total_tasks": 0}
            )
            partner_stats["TMS_ë°˜ì œí’ˆ"]["nan_count"] += tms_nan
            partner_stats["TMS_ë°˜ì œí’ˆ"]["total_tasks"] += tms_total
        elec_nan = partner_stats_individual.get("elec", {}).get("nan_count", 0)
        elec_total = occurrence_stats.get("ì „ì¥", {}).get("total_count", 0)
        if elec_partner:
            partner_stats[elec_partner] = partner_stats.get(
                elec_partner, {"nan_count": 0, "total_tasks": 0}
            )
            partner_stats[elec_partner]["nan_count"] += elec_nan
            partner_stats[elec_partner]["total_tasks"] += elec_total
    nan_counts = [stats["nan_count"] for stats in partner_stats.values()]
    total_nan = sum(nan_counts)
    if total_nan == 0:
        print("âš ï¸ NaN ê±´ìˆ˜ê°€ ì—†ì–´ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None, None
    labels = list(partner_stats.keys())
    nan_ratios = [
        (
            stats["nan_count"] / stats["total_tasks"] * 100
            if stats["total_tasks"] > 0
            else 0
        )
        for stats in partner_stats.values()
    ]
    plt.figure(figsize=(10, 6))
    bars = plt.bar(labels, nan_ratios, color=plt.cm.Paired.colors[: len(labels)])
    plt.title("í˜‘ë ¥ì‚¬ë³„ NaN ë°œìƒ ë¹„ìœ¨ (ì‘ì—… ìˆ˜ ëŒ€ë¹„)", fontsize=14, pad=20)
    plt.xlabel("í˜‘ë ¥ì‚¬", fontsize=12)
    plt.ylabel("NaN ë°œìƒ ë¹„ìœ¨ (%)", fontsize=12)
    plt.xticks(rotation=45, ha="right")
    for i, bar in enumerate(bars):
        height = bar.get_height()
        count = nan_counts[i]
        total = partner_stats[labels[i]]["total_tasks"]
        plt.text(
            bar.get_x() + bar.get_width() / 2.0,
            height,
            f"{height:.1f}%\n({count}/{total})",
            ha="center",
            va="bottom",
            fontsize=10,
        )
    plt.tight_layout()
    tasks_file = "NaN_Summary_by_Tasks.png"
    plt.savefig(tasks_file, bbox_inches="tight")
    plt.close()
    plt.figure(figsize=(8, 8))
    plt.pie(
        nan_counts,
        labels=labels,
        autopct=lambda pct: f"{pct:.1f}% ({int(total_nan * pct / 100)})",
        startangle=90,
        colors=plt.cm.Paired.colors[: len(labels)],
    )
    plt.axis("equal")
    plt.title("í˜‘ë ¥ì‚¬ë³„ NaN ë°œìƒ ë¹„ìœ¨ (ì „ì²´ NaN ê±´ìˆ˜ ëŒ€ë¹„)", fontsize=14, pad=20)
    total_file = "NaN_Summary_by_Total.png"
    plt.savefig(total_file, bbox_inches="tight")
    plt.close()
    print(f"ğŸ“Š ë§‰ëŒ€ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ: {tasks_file}, íŒŒì´ì°¨íŠ¸ ìƒì„± ì™„ë£Œ: {total_file}")
    return tasks_file, total_file


# Email & Notification Functions
def render_progress_bar(percent, total, label):
    completed = round((percent / 100) * total)
    tooltip = f"{label} ì™„ë£Œ: {completed} / {total}ê±´"
    if percent == 100:
        return f'<span title="{tooltip}" style="font-size: 16px;">âœ…</span>'
    else:
        return (
            f'<div style="width: 100%; background-color: #e0e0e0; height: 12px; border-radius: 3px;">'
            f'<div style="width: {percent}%; background-color: orange; height: 100%; border-radius: 3px;" title="{tooltip}"></div></div>'
            f'<span style="font-size: 12px;">{percent:.1f}%</span>'
        )


def build_combined_email_body(
    all_results,
    nan_tasks_link=None,
    nan_total_link=None,
    heatmap_url=None,
    monthly_partner_url=None,
    monthly_model_url=None,
):
    kst = pytz.timezone("Asia/Seoul")
    execution_time = datetime.now(kst).strftime("%Y-%m-%d %H:%M:%S")
    year, week_num, _ = date.today().isocalendar()
    dashboard_link = os.getenv("DASHBOARD_URL", "https://gst-factory.netlify.app")

    # ê³ ìœ  ê°’ ìˆ˜ì§‘ (í•„í„° ë“œë¡­ë‹¤ìš´ìš©)
    unique_values = {
        "Order": set(),
        "ëª¨ë¸ëª…": set(),
        "ê¸°êµ¬í˜‘ë ¥ì‚¬": set(),
        "ì „ì¥í˜‘ë ¥ì‚¬": set(),
        "ì´ ì‘ì—… ìˆ˜": set(),
        "ê¸°êµ¬ NaN": set(),
        "ê¸°êµ¬ OT": set(),
        "ê¸°êµ¬ ì§„í–‰ë¥ ": set(),
        "ì „ì¥ NaN": set(),
        "ì „ì¥ OT": set(),
        "ì „ì¥ ì§„í–‰ë¥ ": set(),
        "TMS NaN": set(),
        "TMS OT": set(),
        "TMS ì§„í–‰ë¥ ": set(),
    }
    for (
        order_no,
        model_name,
        mech_partner,
        elec_partner,
        occurrence_stats,
        partner_stats,
        _,
        spreadsheet_url,
        progress_summary,
    ) in all_results:
        total_tasks = sum(stats["total_count"] for stats in occurrence_stats.values())
        mech_stats = partner_stats.get("mech", {})
        elec_stats = partner_stats.get("elec", {})
        tms_stats = occurrence_stats.get("TMS_ë°˜ì œí’ˆ", {})
        unique_values["Order"].add(order_no)
        unique_values["ëª¨ë¸ëª…"].add(model_name)
        unique_values["ê¸°êµ¬í˜‘ë ¥ì‚¬"].add(mech_partner)
        unique_values["ì „ì¥í˜‘ë ¥ì‚¬"].add(elec_partner)
        unique_values["ì´ ì‘ì—… ìˆ˜"].add(str(total_tasks))
        unique_values["ê¸°êµ¬ NaN"].add(str(mech_stats.get("nan_count", 0)))
        unique_values["ê¸°êµ¬ OT"].add(str(mech_stats.get("ot_count", 0)))
        unique_values["ì „ì¥ NaN"].add(str(elec_stats.get("nan_count", 0)))
        unique_values["ì „ì¥ OT"].add(str(elec_stats.get("ot_count", 0)))
        unique_values["TMS NaN"].add(str(tms_stats.get("nan_count", 0)))
        unique_values["TMS OT"].add(str(tms_stats.get("ot_count", 0)))
        # ì§„í–‰ë¥  ê³ ìœ  ê°’ ì¶”ê°€ (ì†Œìˆ˜ì  1ìë¦¬ ë¬¸ìì—´)
        prog = progress_summary or {"ê¸°êµ¬": 0, "ì „ì¥": 0, "TMS_ë°˜ì œí’ˆ": 0}
        unique_values["ê¸°êµ¬ ì§„í–‰ë¥ "].add(f"{prog.get('ê¸°êµ¬', 0):.1f}")
        unique_values["ì „ì¥ ì§„í–‰ë¥ "].add(f"{prog.get('ì „ì¥', 0):.1f}")
        unique_values["TMS ì§„í–‰ë¥ "].add(f"{prog.get('TMS_ë°˜ì œí’ˆ', 0):.1f}")

    lines = [
        '<div style="text-align: center; margin-bottom: 20px;">' "</div>",
        '<div class="chart-section" style="margin-top: 30px;">',
        '<iframe src="partner_entry_chart.html" width="100%" height="1200" frameborder="0"></iframe>',
        "</div>",
        f"<h1>PDA Dashboard - {year}ë…„ {week_num}ì£¼ì°¨</h1>",
        f"<h3>ğŸ“Œ [ì•Œë¦¼] PDA Overtime ë° NaN ì²´í¬ ê²°ê³¼ (ì´ {len(all_results)}ê±´ ì²˜ë¦¬)</h3>",
        f"<p>ğŸ“… ì‹¤í–‰ ì‹œê°„: {execution_time} (KST)</p>",
        f'<p>ğŸ“Š ëŒ€ì‹œë³´ë“œì—ì„œ ìƒì„¸ ë‚´ìš© í™•ì¸í•˜ì„¸ìš”! (<a href="{dashboard_link}">ëŒ€ì‹œë³´ë“œ ë°”ë¡œê°€ê¸°</a>',
    ]

    # NOVA íŠ¸ë Œë“œ ê·¸ë˜í”„ ì„¹ì…˜ì„ ëŒ€ì‹œë³´ë“œ ë§í¬ ë°”ë¡œ ë’¤ì— ì¶”ê°€
    nova_links = []
    if heatmap_url:
        nova_links.append(
            f'ğŸ“… ì£¼ê°„ í˜‘ë ¥ì‚¬ NaN íˆíŠ¸ë§µ: <a href="{heatmap_url}" target="_blank">ê·¸ë˜í”„ ë³´ê¸°</a>'
        )

    # ì›”ê°„ íˆíŠ¸ë§µ ë§í¬ë“¤ (Google Driveì—ì„œ ìµœì‹  íŒŒì¼ ê²€ìƒ‰)
    if not monthly_partner_url:
        # Google Driveì—ì„œ ìµœì‹  ì›”ê°„ í˜‘ë ¥ì‚¬ íˆíŠ¸ë§µ íŒŒì¼ ê²€ìƒ‰
        try:
            query = f"'{DRIVE_FOLDER_ID}' in parents and name contains 'monthly_partner_nan_heatmap_'"
            files = (
                drive_service.files()
                .list(q=query, fields="files(id, name)")
                .execute()
                .get("files", [])
            )

            if files:
                # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œí•˜ì—¬ ìµœì‹  íŒŒì¼ ì„ íƒ
                latest_partner_file = max(
                    files, key=lambda x: x["name"].split("_")[-1].replace(".png", "")
                )
                monthly_partner_url = f"https://drive.google.com/uc?export=view&id={latest_partner_file['id']}"
                print(
                    f"ğŸ“ Driveì—ì„œ ìµœì‹  ì›”ê°„ í˜‘ë ¥ì‚¬ íˆíŠ¸ë§µ ë°œê²¬: {latest_partner_file['name']}"
                )
                print(f"âœ… ì›”ê°„ í˜‘ë ¥ì‚¬ íˆíŠ¸ë§µ URL: {monthly_partner_url}")
            else:
                print("âš ï¸ Driveì—ì„œ ì›”ê°„ í˜‘ë ¥ì‚¬ íˆíŠ¸ë§µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ Drive ê²€ìƒ‰ ì˜¤ë¥˜ (í˜‘ë ¥ì‚¬): {e}")

        # Driveì—ì„œ ì°¾ì§€ ëª»í•˜ë©´ í™˜ê²½ë³€ìˆ˜ ê¸°ë³¸ê°’ ì‚¬ìš©
        if not monthly_partner_url:
            monthly_partner_url = os.getenv(
                "MONTHLY_PARTNER_HEATMAP_URL",
                "https://drive.google.com/uc?export=view&id=1Bh1iUvPIQfsQ_wUTs_DOln0cZGY_hHL7",
            )
            print(f"âš ï¸ Drive ê²€ìƒ‰ ì‹¤íŒ¨, í™˜ê²½ë³€ìˆ˜ URL ì‚¬ìš©: {monthly_partner_url}")

    if not monthly_model_url:
        # Google Driveì—ì„œ ìµœì‹  ì›”ê°„ ëª¨ë¸ íˆíŠ¸ë§µ íŒŒì¼ ê²€ìƒ‰
        try:
            query = f"'{DRIVE_FOLDER_ID}' in parents and name contains 'monthly_model_nan_heatmap_'"
            files = (
                drive_service.files()
                .list(q=query, fields="files(id, name)")
                .execute()
                .get("files", [])
            )

            if files:
                # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œí•˜ì—¬ ìµœì‹  íŒŒì¼ ì„ íƒ
                latest_model_file = max(
                    files, key=lambda x: x["name"].split("_")[-1].replace(".png", "")
                )
                monthly_model_url = f"https://drive.google.com/uc?export=view&id={latest_model_file['id']}"
                print(
                    f"ğŸ“ Driveì—ì„œ ìµœì‹  ì›”ê°„ ëª¨ë¸ íˆíŠ¸ë§µ ë°œê²¬: {latest_model_file['name']}"
                )
                print(f"âœ… ì›”ê°„ ëª¨ë¸ íˆíŠ¸ë§µ URL: {monthly_model_url}")
            else:
                print("âš ï¸ Driveì—ì„œ ì›”ê°„ ëª¨ë¸ íˆíŠ¸ë§µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ Drive ê²€ìƒ‰ ì˜¤ë¥˜ (ëª¨ë¸): {e}")

        # Driveì—ì„œ ì°¾ì§€ ëª»í•˜ë©´ í™˜ê²½ë³€ìˆ˜ ê¸°ë³¸ê°’ ì‚¬ìš©
        if not monthly_model_url:
            monthly_model_url = os.getenv(
                "MONTHLY_MODEL_HEATMAP_URL",
                "https://drive.google.com/uc?export=view&id=1DGOJCR5Ie5VGgMMcgIEQc0D45z8-uuIG",
            )
            print(f"âš ï¸ Drive ê²€ìƒ‰ ì‹¤íŒ¨, í™˜ê²½ë³€ìˆ˜ URL ì‚¬ìš©: {monthly_model_url}")

    # ë§í¬ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
    if monthly_partner_url:
        nova_links.append(
            f'ğŸ—“ï¸ ì›”ê°„ í˜‘ë ¥ì‚¬ NaN íˆíŠ¸ë§µ: <a href="{monthly_partner_url}" target="_blank">ê·¸ë˜í”„ ë³´ê¸°</a>'
        )
    if monthly_model_url:
        nova_links.append(
            f'ğŸ“ˆ ì›”ê°„ ëª¨ë¸ë³„ NaN íˆíŠ¸ë§µ: <a href="{monthly_model_url}" target="_blank">ê·¸ë˜í”„ ë³´ê¸°</a>'
        )

    if nova_links:
        lines.append("<p><strong>ğŸ“ŠíŠ¸ë Œë“œ ì§€í‘œ</strong></p><ul>")
        for link in nova_links:
            lines.append(f"<li>{link}</li>")
        lines.append("</ul>")

    lines.append(")</p>")
    lines.extend(
        [
            "<h4>ìš”ì•½ í…Œì´ë¸”</h4>",
            '<table id="summaryTable" border="1" style="border-collapse: collapse; width: 95%; font-size: 13px;">',
            '<tr style="background-color: #f2f2f2;">',
        ]
    )

    # í—¤ë” ë° í•„í„° ë“œë¡­ë‹¤ìš´ ì¶”ê°€
    headers = [
        "Order",
        "ëª¨ë¸ëª…",
        "ê¸°êµ¬í˜‘ë ¥ì‚¬",
        "ì „ì¥í˜‘ë ¥ì‚¬",
        "ì´ ì‘ì—… ìˆ˜",
        "ê¸°êµ¬ NaN",
        "ê¸°êµ¬ OT",
        "ê¸°êµ¬ ì§„í–‰ë¥ ",
        "ì „ì¥ NaN",
        "ì „ì¥ OT",
        "ì „ì¥ ì§„í–‰ë¥ ",
        "TMS NaN",
        "TMS OT",
        "TMS ì§„í–‰ë¥ ",
    ]
    for header in headers:
        lines.append(
            f'<th data-column="{header}">{header}<br>'
            f'<select onchange="filterTable(\'{header}\')" style="width: 100%; font-size: 12px;">'
            f'<option value="">ì „ì²´</option>'
        )
        for value in sorted(unique_values[header]):
            lines.append(f'<option value="{value}">{value}</option>')
        lines.append("</select></th>")
    lines.append("</tr>")

    # í…Œì´ë¸” í–‰ ìƒì„± (Order ì—´ì— í•˜ì´í¼ë§í¬ ì¶”ê°€)
    for (
        order_no,
        model_name,
        mech_partner,
        elec_partner,
        occurrence_stats,
        partner_stats,
        links,
        spreadsheet_url,
        progress_summary,
    ) in all_results:
        total_tasks = sum(stats["total_count"] for stats in occurrence_stats.values())
        prog = progress_summary or {"ê¸°êµ¬": 0, "ì „ì¥": 0, "TMS_ë°˜ì œí’ˆ": 0}
        prog_mech = prog.get("ê¸°êµ¬", 0)
        prog_elec = prog.get("ì „ì¥", 0)
        prog_tms = prog.get("TMS_ë°˜ì œí’ˆ", 0)
        mech_stats = partner_stats.get("mech", {})
        elec_stats = partner_stats.get("elec", {})
        tms_stats = occurrence_stats.get("TMS_ë°˜ì œí’ˆ", {})
        mech_nan = mech_stats.get("nan_count", 0)
        mech_ot = mech_stats.get("ot_count", 0)
        mech_total = mech_stats.get("total_count", 0)
        elec_nan = elec_stats.get("nan_count", 0)
        elec_ot = elec_stats.get("ot_count", 0)
        elec_total = elec_stats.get("total_count", 0)
        tms_nan = tms_stats.get("nan_count", 0)
        tms_ot = tms_stats.get("ot_count", 0)
        tms_total = tms_stats.get("total_count", 0)
        lines.append(
            f'<tr><td><a href="{spreadsheet_url}">{order_no}</a></td><td>{model_name}</td><td>{mech_partner}</td><td>{elec_partner}</td><td>{total_tasks}</td>'
            f'<td{" style=" + chr(34) + "color: red; font-weight: bold;" + chr(34) if mech_nan > 0 else ""}>{mech_nan}</td>'
            f'<td{" style=" + chr(34) + "color: red; font-weight: bold;" + chr(34) if mech_ot > 0 else ""}>{mech_ot}</td>'
            f'<td>{render_progress_bar(prog_mech, mech_total, "ê¸°êµ¬")}</td>'
            f'<td{" style=" + chr(34) + "color: red; font-weight: bold;" + chr(34) if elec_nan > 0 else ""}>{elec_nan}</td>'
            f'<td{" style=" + chr(34) + "color: red; font-weight: bold;" + chr(34) if elec_ot > 0 else ""}>{elec_ot}</td>'
            f'<td>{render_progress_bar(prog_elec, elec_total, "ì „ì¥")}</td>'
            f'<td{" style=" + chr(34) + "color: red; font-weight: bold;" + chr(34) if tms_nan > 0 else ""}>{tms_nan}</td>'
            f'<td{" style=" + chr(34) + "color: red; font-weight: bold;" + chr(34) if tms_ot > 0 else ""}>{tms_ot}</td>'
            f'<td>{render_progress_bar(prog_tms, tms_total, "TMS")}</td></tr>'
        )
    lines.append("</table><br>")

    # JavaScript í•„í„°ë§ ë¡œì§ (ì§„í–‰ë¥  ì²˜ë¦¬ í¬í•¨)
    lines.append(
        """
    <script>
    function filterTable(column) {
        const table = document.getElementById("summaryTable");
        const select = table.querySelector(`th[data-column="${column}"] select`);
        const filterValue = select.value.toLowerCase();
        const rows = table.getElementsByTagName("tr");

        for (let i = 1; i < rows.length; i++) { // Skip header row
            const cells = rows[i].getElementsByTagName("td");
            const cellIndex = Array.from(table.getElementsByTagName("th")).findIndex(th => th.dataset.column === column);
            let cellValue = cells[cellIndex].innerText.toLowerCase();
            // ì§„í–‰ë¥  ì—´ì˜ ê²½ìš° ìˆ«ì ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì˜ˆ: "85.7%" â†’ "85.7")
            if (column.includes("ì§„í–‰ë¥ ")) {
                const match = cellValue.match(/\\d+\\.\\d/); // ìˆ«ì.ì†Œìˆ˜ì  íŒ¨í„´
                cellValue = match ? match[0] : "0.0";
            }
            // Order ì—´ì˜ ê²½ìš° í•˜ì´í¼ë§í¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if (column === "Order") {
                cellValue = cells[cellIndex].querySelector("a") ? cells[cellIndex].querySelector("a").innerText.toLowerCase() : cellValue;
            }
            let showRow = true;

            // ëª¨ë“  í•„í„° í™•ì¸
            table.querySelectorAll("th select").forEach(sel => {
                const col = sel.parentElement.dataset.column;
                const val = sel.value.toLowerCase();
                if (val) {
                    const idx = Array.from(table.getElementsByTagName("th")).findIndex(th => th.dataset.column === col);
                    let compareValue = cells[idx].innerText.toLowerCase();
                    if (col.includes("ì§„í–‰ë¥ ")) {
                        const match = compareValue.match(/\\d+\\.\\d/);
                        compareValue = match ? match[0] : "0.0";
                    }
                    if (col === "Order") {
                        compareValue = cells[idx].querySelector("a") ? cells[idx].querySelector("a").innerText.toLowerCase() : compareValue;
                    }
                    if (compareValue !== val) {
                        showRow = false;
                    }
                }
            });

            rows[i].style.display = showRow ? "" : "none";
        }
    }
    </script>
    """
    )

    for (
        order_no,
        model_name,
        mech_partner,
        elec_partner,
        occurrence_stats,
        partner_stats,
        links,
        spreadsheet_url,
        progress_summary,
    ) in all_results:
        lines.append(
            f"<details><summary><strong>ğŸ“ Order: {order_no}, ëª¨ë¸ëª…: {model_name}</strong></summary>"
        )
        lines.append(
            f"<p>ğŸ­ ê¸°êµ¬í˜‘ë ¥ì‚¬: {mech_partner}, âš¡ ì „ì¥í˜‘ë ¥ì‚¬: {elec_partner}</p>"
        )
        lines.append(
            f'<p>ğŸ“‹ <strong>ëª¨ë¸ ìŠ¤í”„ë ˆë“œì‹œíŠ¸</strong>: <a href="{spreadsheet_url}">ë°”ë¡œê°€ê¸°</a></p>'
        )
        lines.append(
            f'<p>ğŸ“Š ëŒ€ì‹œë³´ë“œ ë§í¬: <a href="{dashboard_link}">ëŒ€ì‹œë³´ë“œ ë°”ë¡œê°€ê¸°</a></p>'
        )
        lines.append(
            f"<p>ğŸ“Š ê·¸ë˜í”„ ë§í¬:</p><ul>"
            f'<li>Working Hours: <a href="{links["working_hours"]}">ë°”ë¡œê°€ê¸°</a></li>'
            f'<li>Legend Chart: <a href="{links["legend"]}">ë°”ë¡œê°€ê¸°</a></li>'
            f'<li>WD Chart: <a href="{links["wd"]}">ë°”ë¡œê°€ê¸°</a></li></ul>'
        )
        for category in ["ê¸°êµ¬", "TMS_ë°˜ì œí’ˆ", "ì „ì¥", "ê²€ì‚¬", "ë§ˆë¬´ë¦¬", "ê¸°íƒ€"]:
            stats = occurrence_stats.get(
                category,
                {
                    "total_count": 0,
                    "nan_count": 0,
                    "ot_count": 0,
                    "nan_tasks": [],
                    "ot_task_details": [],
                },
            )
            total_count = stats["total_count"]
            lines.append(
                f"<p><b>ğŸ”¹ {category} ì‘ì—…</b><br> - ì „ì²´ ì‘ì—… ìˆ˜: {total_count} ê±´<br>"
            )
            nan_count = stats["nan_count"]
            nan_ratio = (nan_count / total_count) * 100 if total_count > 0 else 0
            lines.append(
                f' <span{" style=" + chr(34) + "color: red;" + chr(34) if nan_count > 0 else ""}>âš ï¸ ëˆ„ë½(NaN): {nan_count} ê±´ (ë¹„ìœ¨: {nan_ratio:.2f}%)</span><br>'
            )
            if nan_count > 0:
                lines.append("".join(f"   - {task}<br>" for task in stats["nan_tasks"]))
            ot_count = stats["ot_count"]
            ot_ratio = (ot_count / total_count) * 100 if total_count > 0 else 0
            lines.append(
                f' <span{" style=" + chr(34) + "color: red;" + chr(34) if ot_count > 0 else ""}>â³ ì˜¤ë²„íƒ€ì„: {ot_count} ê±´ (ë¹„ìœ¨: {ot_ratio:.2f}%)</span><br>'
            )
            if ot_count > 0:
                lines.append(
                    "".join(
                        f"   - {task} {format_hours(hours)}<br>"
                        for task, hours in stats["ot_task_details"]
                    )
                )
            lines.append("</p>")
        lines.append("</details><hr>")

    return "\n".join(lines)


def send_occurrence_email(subject, body_text, graph_files=None, dashboard_file=None):
    # ì´ë©”ì¼ ì„¤ì •ì´ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
    if not email_configured:
        print(f"âš ï¸ ì´ë©”ì¼ ì„¤ì •ì´ ì—†ì–´ ì´ë©”ì¼ ì „ì†¡ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        print(f"ğŸ“§ ì œëª©: {subject}")
        return

    context = ssl.create_default_context()
    msg = MIMEMultipart("mixed")
    msg["From"] = EMAIL_ADDRESS
    msg["To"] = RECEIVER_EMAIL
    msg["Subject"] = subject
    msg.attach(MIMEText(body_text, "html", _charset="utf-8"))
    if graph_files:
        for graph_file in graph_files:
            try:
                with open(graph_file, "rb") as f:
                    img = MIMEImage(f.read())
                    img.add_header(
                        "Content-Disposition", "attachment", filename=graph_file
                    )
                    msg.attach(img)
                print(f"âœ… ê·¸ë˜í”„ íŒŒì¼ ì²¨ë¶€ ì™„ë£Œ: {graph_file}")
            except Exception as e:
                print(f"âŒ [ì²¨ë¶€ ì˜¤ë¥˜] ê·¸ë˜í”„ íŒŒì¼ {graph_file} ì²¨ë¶€ ì‹¤íŒ¨: {e}")
    if dashboard_file:
        try:
            with open(dashboard_file, "rb") as f:
                html_attachment = MIMEApplication(f.read(), _subtype="html")
                html_attachment.add_header(
                    "Content-Disposition", "attachment", filename=dashboard_file
                )
                msg.attach(html_attachment)
            print(f"ğŸ“„ [ì´ë©”ì¼ ì²¨ë¶€] HTML ëŒ€ì‹œë³´ë“œ íŒŒì¼ {dashboard_file} ì¶”ê°€ ì™„ë£Œ")
        except Exception as e:
            print(
                f"âŒ [ì´ë©”ì¼ ì²¨ë¶€ ì˜¤ë¥˜] HTML ëŒ€ì‹œë³´ë“œ íŒŒì¼ {dashboard_file} ì¶”ê°€ ì‹¤íŒ¨: {e}"
            )
    try:
        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:
            server.starttls(context=context)
            server.login(EMAIL_ADDRESS, EMAIL_PASS)
            server.send_message(msg)
        print(
            f"ğŸ“§ [ì´ë©”ì¼ ë°œì†¡] {RECEIVER_EMAIL}ë¡œ í†µí•© HTML ì•Œë¦¼ ë©”ì¼ ë° ì²¨ë¶€íŒŒì¼ ì „ì†¡ ì™„ë£Œ"
        )
    except Exception as e:
        print(f"âŒ [ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨]: {e}")


def cross_check_data_integrity(all_results):
    """
    ê¸°ì¡´ ê²°ê³¼ê°’ë“¤ì„ í¬ë¡œìŠ¤ ì²´í¬í•˜ì—¬ ë°ì´í„° ì •í•©ì„± í™•ì¸

    Args:
        all_results: ì²˜ë¦¬ëœ ëª¨ë“  ê²°ê³¼ ë°ì´í„°

    Returns:
        dict: í¬ë¡œìŠ¤ ì²´í¬ ê²°ê³¼ ë° ê²½ê³  ë©”ì‹œì§€
    """
    check_report = {
        "total_models": len(all_results),
        "warnings": [],
        "summary": {
            "total_nan_by_category": 0,
            "total_nan_by_partner": 0,
            "total_ot_by_category": 0,
            "total_ot_by_partner": 0,
            "category_breakdown": {},
            "partner_breakdown": {},
        },
    }

    # ì¹´í…Œê³ ë¦¬ë³„ ì´í•© ê³„ì‚° (ê¸°ì¡´ ë°©ì‹)
    category_nan_total = 0
    category_ot_total = 0
    category_breakdown = {}

    # í˜‘ë ¥ì‚¬ë³„ ì´í•© ê³„ì‚°
    partner_nan_total = 0
    partner_ot_total = 0
    partner_breakdown = {"mech": 0, "elec": 0}

    for result in all_results:
        (
            order_no,
            model_name,
            mech_partner,
            elec_partner,
            occurrence_stats,
            partner_stats,
            _,
            _,
            _,
        ) = result

        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ì§‘ê³„
        for category, stats in occurrence_stats.items():
            nan_count = stats.get("nan_count", 0)
            ot_count = stats.get("ot_count", 0)

            category_nan_total += nan_count
            category_ot_total += ot_count

            if category not in category_breakdown:
                category_breakdown[category] = {"nan": 0, "ot": 0}
            category_breakdown[category]["nan"] += nan_count
            category_breakdown[category]["ot"] += ot_count

        # í˜‘ë ¥ì‚¬ë³„ í†µê³„ ì§‘ê³„
        for partner_type, stats in partner_stats.items():
            nan_count = stats.get("nan_count", 0)
            ot_count = stats.get("ot_count", 0)

            partner_nan_total += nan_count
            partner_ot_total += ot_count

            if partner_type in partner_breakdown:
                partner_breakdown[partner_type] += nan_count

    # í¬ë¡œìŠ¤ ì²´í¬ 1: ê¸°êµ¬/ì „ì¥ ì¹´í…Œê³ ë¦¬ì™€ í˜‘ë ¥ì‚¬ í†µê³„ ë¹„êµ
    mech_category_nan = category_breakdown.get("ê¸°êµ¬", {}).get("nan", 0)
    elec_category_nan = category_breakdown.get("ì „ì¥", {}).get("nan", 0)

    mech_partner_nan = partner_breakdown.get("mech", 0)
    elec_partner_nan = partner_breakdown.get("elec", 0)

    if mech_category_nan != mech_partner_nan:
        check_report["warnings"].append(
            f"âš ï¸ ê¸°êµ¬ NaN ë¶ˆì¼ì¹˜: ì¹´í…Œê³ ë¦¬({mech_category_nan}) â‰  í˜‘ë ¥ì‚¬({mech_partner_nan})"
        )

    if elec_category_nan != elec_partner_nan:
        check_report["warnings"].append(
            f"âš ï¸ ì „ì¥ NaN ë¶ˆì¼ì¹˜: ì¹´í…Œê³ ë¦¬({elec_category_nan}) â‰  í˜‘ë ¥ì‚¬({elec_partner_nan})"
        )

    # í¬ë¡œìŠ¤ ì²´í¬ 2: ì¹´í…Œê³ ë¦¬ë³„ ì£¼ìš” í†µê³„ í™•ì¸
    major_categories = ["ê¸°êµ¬", "ì „ì¥", "TMS_ë°˜ì œí’ˆ"]
    for category in major_categories:
        if category in category_breakdown:
            nan_count = category_breakdown[category]["nan"]
            total_count = sum(
                stats.get("total_count", 0)
                for result in all_results
                for stats in [result[4].get(category, {})]
            )

            if total_count > 0:
                nan_ratio = (nan_count / total_count) * 100
                if nan_ratio > 80:  # 80% ì´ìƒì´ë©´ ê²½ê³ 
                    check_report["warnings"].append(
                        f"âš ï¸ {category} NaN ë¹„ìœ¨ ë†’ìŒ: {nan_ratio:.1f}% ({nan_count}/{total_count})"
                    )

    # í¬ë¡œìŠ¤ ì²´í¬ 3: ì „ì²´ ëª¨ë¸ ìˆ˜ì™€ ì‹¤ì œ ì²˜ë¦¬ëœ ë°ì´í„° ìˆ˜ ë¹„êµ
    processed_models = len(
        [
            r
            for r in all_results
            if any(
                stats.get("nan_count", 0) > 0 or stats.get("ot_count", 0) > 0
                for stats in r[4].values()
            )
        ]
    )

    if processed_models != len(all_results):
        check_report["warnings"].append(
            f"â„¹ï¸ ì²˜ë¦¬ëœ ëª¨ë¸: {processed_models}/{len(all_results)} (ì •ìƒ ë²”ìœ„ ë‚´ ëª¨ë¸ ì œì™¸)"
        )

    # ìš”ì•½ ì •ë³´ ì—…ë°ì´íŠ¸
    check_report["summary"]["total_nan_by_category"] = category_nan_total
    check_report["summary"]["total_nan_by_partner"] = partner_nan_total
    check_report["summary"]["total_ot_by_category"] = category_ot_total
    check_report["summary"]["total_ot_by_partner"] = partner_ot_total
    check_report["summary"]["category_breakdown"] = category_breakdown
    check_report["summary"]["partner_breakdown"] = partner_breakdown

    return check_report


def send_nan_alert_to_kakao(all_results):
    if not all_results:
        print("âš ï¸ [ì•Œë¦¼] ì „ì†¡í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ê¸°ì¡´ ê²°ê³¼ê°’ë“¤ì„ í¬ë¡œìŠ¤ ì²´í¬
    print("ğŸ” ê¸°ì¡´ ê²°ê³¼ê°’ í¬ë¡œìŠ¤ ì²´í¬ ì¤‘...")
    check_report = cross_check_data_integrity(all_results)

    # í¬ë¡œìŠ¤ ì²´í¬ ê²°ê³¼ ì¶œë ¥
    if check_report["warnings"]:
        print("âš ï¸ ë°ì´í„° í¬ë¡œìŠ¤ ì²´í¬ ê²°ê³¼:")
        for warning in check_report["warnings"]:
            print(f"  {warning}")
    else:
        print("âœ… ë°ì´í„° í¬ë¡œìŠ¤ ì²´í¬ í†µê³¼")

    # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì´í•© ê³„ì‚° (ê²€ì¦ëœ ê°’ ì‚¬ìš©)
    total_nan = check_report["summary"]["total_nan_by_category"]
    total_ot = check_report["summary"]["total_ot_by_category"]

    # ê¸°ì¡´ ë°©ì‹ê³¼ ë¹„êµ (ë””ë²„ê¹…ìš©)
    original_nan = sum(
        stats["nan_count"] for result in all_results for stats in result[4].values()
    )
    original_ot = sum(
        stats["ot_count"] for result in all_results for stats in result[4].values()
    )

    if total_nan != original_nan or total_ot != original_ot:
        print(
            f"âš ï¸ ê³„ì‚° ë°©ì‹ ì°¨ì´ ë°œê²¬: NaN({total_nan}vs{original_nan}), OT({total_ot}vs{original_ot})"
        )
        # ê¸°ì¡´ ë°©ì‹ ì‚¬ìš© (ì•ˆì „)
        total_nan = original_nan
        total_ot = original_ot

    kst = pytz.timezone("Asia/Seoul")
    execution_time = datetime.now(kst).strftime("%Y-%m-%d %H:%M:%S")

    # ì¹´ì¹´ì˜¤í†¡ ë©”ì‹œì§€ êµ¬ì„±
    text = f"ğŸ“¢ PDA Overtime ë° NaN ì²´í¬ ê²°ê³¼\nğŸ“… ì‹¤í–‰ ì‹œê°„: {execution_time} (KST)\nğŸ“Š ì´ {len(all_results)}ê±´ ì²˜ë¦¬\nâš ï¸ ëˆ„ë½(NaN): {total_nan} ê±´\nâ³ ì˜¤ë²„íƒ€ì„: {total_ot} ê±´"

    # í¬ë¡œìŠ¤ ì²´í¬ ê²½ê³ ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€ ì •ë³´
    if check_report["warnings"]:
        text += f"\nğŸ” ë°ì´í„° ì²´í¬: {len(check_report['warnings'])}ê±´ í™•ì¸ í•„ìš”"

    # ì£¼ìš” ì¹´í…Œê³ ë¦¬ ìš”ì•½ ì¶”ê°€
    category_summary = []
    for category in ["ê¸°êµ¬", "ì „ì¥", "TMS_ë°˜ì œí’ˆ"]:
        if category in check_report["summary"]["category_breakdown"]:
            nan_count = check_report["summary"]["category_breakdown"][category]["nan"]
            if nan_count > 0:
                category_summary.append(f"{category}: {nan_count}ê±´")

    if category_summary:
        text += f"\nğŸ“‹ ì£¼ìš” ëˆ„ë½: {', '.join(category_summary)}"

    text += "\nğŸ‘‡ ëŒ€ì‹œë³´ë“œì—ì„œ ìƒì„¸ ë‚´ìš© í™•ì¸í•˜ì„¸ìš”!"

    # ë©”ì‹œì§€ ì „ì†¡
    access_token = refresh_access_token()
    if not access_token:
        print("âŒ [ì¹´ì¹´ì˜¤í†¡ ë°œì†¡ ì‹¤íŒ¨] ì•¡ì„¸ìŠ¤ í† í°ì´ ì—†ì–´ ë©”ì‹œì§€ ë°œì†¡ ë¶ˆê°€.")
        return

    success = send_kakao_message(text, access_token)

    if success:
        print("âœ… í¬ë¡œìŠ¤ ì²´í¬ ì™„ë£Œ ë° ì¹´ì¹´ì˜¤í†¡ ë©”ì‹œì§€ ì „ì†¡ ì„±ê³µ!")
        print(f"ğŸ“Š ê²€ì¦ëœ í†µê³„: NaN {total_nan}ê±´, OT {total_ot}ê±´")
        if check_report["warnings"]:
            print(f"âš ï¸ í™•ì¸ í•„ìš” í•­ëª©: {len(check_report['warnings'])}ê±´")
    else:
        print("âŒ ì¹´ì¹´ì˜¤í†¡ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨")


def refresh_access_token():
    url = "https://kauth.kakao.com/oauth/token"
    data = {
        "grant_type": "refresh_token",
        "client_id": REST_API_KEY,
        "refresh_token": REFRESH_TOKEN,
    }
    try:
        response = requests.post(url, data=data)
        response.raise_for_status()
        token_info = response.json()
        if "access_token" in token_info:
            new_access_token = token_info["access_token"]
            print(f"âœ… ìƒˆ ì•¡ì„¸ìŠ¤ í† í°: {new_access_token}")
            return new_access_token
        else:
            print(f"âŒ í† í° ê°±ì‹  ì‹¤íŒ¨: {token_info}")
            return None
    except requests.exceptions.RequestException as e:
        print(f"âŒ [ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜] í† í° ê°±ì‹  ì‹¤íŒ¨: {e}")
        return None


def send_kakao_message(text, access_token=None):
    if access_token is None:
        try:
            access_token = KAKAO_ACCESS_TOKEN
        except NameError:
            print("âŒ [ì˜¤ë¥˜] KAKAO_ACCESS_TOKENì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
    url = "https://kapi.kakao.com/v2/api/talk/memo/default/send"
    headers = {
        "Authorization": f"Bearer {access_token}",
        "Content-Type": "application/x-www-form-urlencoded",
    }
    data = {
        "template_object": json.dumps(
            {
                "object_type": "text",
                "text": text,
                "link": {"web_url": "", "mobile_web_url": ""},
            },
            ensure_ascii=False,
        )
    }
    try:
        response = requests.post(url, headers=headers, data=data)
        response.raise_for_status()
        print(
            f"âœ… ì¹´ì¹´ì˜¤í†¡ ë©”ì‹œì§€ ì „ì†¡ ì„±ê³µ! (ì‹œê°„: {datetime.now().strftime('%H:%M:%S')})"
        )
        return True
    except Exception as e:
        print(f"âŒ [ì¹´ì¹´ì˜¤í†¡ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨]: {e}")
        return False


def upload_to_github(file_path):
    GITHUB_USERNAME = os.getenv("GITHUB_USERNAME", "isolhsolfafa")
    GITHUB_REPO = os.getenv("GITHUB_REPO", "gst-factory")
    GITHUB_BRANCH = os.getenv("GITHUB_BRANCH", "main")
    GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
    if not GITHUB_TOKEN:
        print("âŒ [ì˜¤ë¥˜] GITHUB_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    with open(file_path, "rb") as file:
        content = base64.b64encode(file.read()).decode("utf-8")
    file_name = os.path.basename(file_path)
    url = f"https://api.github.com/repos/{GITHUB_USERNAME}/{GITHUB_REPO}/contents/public/{file_name}"
    headers = {
        "Authorization": f"Bearer {GITHUB_TOKEN}",
        "Accept": "application/vnd.github.v3+json",
    }
    response = requests.get(url, headers=headers)
    sha = response.json().get("sha", "")
    data = {
        "message": f"ìë™ ì—…ë¡œë“œ: {file_name}",
        "content": content,
        "branch": GITHUB_BRANCH,
    }
    if sha:
        data["sha"] = sha
    response = requests.put(url, headers=headers, json=data)
    if response.status_code in [200, 201]:
        print(f"âœ… {file_name} GitHub ì—…ë¡œë“œ ì„±ê³µ!")
    else:
        print(f"âŒ {file_name} GitHub ì—…ë¡œë“œ ì‹¤íŒ¨! {response.text}")


def get_mech_start_date(spreadsheet_url, sheets_service):
    try:
        match = re.search(r"/d/([a-zA-Z0-9-_]+)", spreadsheet_url)
        if not match:
            return pd.NaT
        spreadsheet_id = match.group(1)
        result = api_call_with_backoff(
            sheets_service.spreadsheets().values().get,
            spreadsheetId=spreadsheet_id,
            range="ì •ë³´íŒ!B6",
            valueRenderOption="FORMATTED_VALUE",
        ).execute()
        raw_date = result.get("values", [[]])[0][0]
        return pd.to_datetime(raw_date, errors="coerce")
    except Exception as e:
        print(f"âŒ [ì˜¤ë¥˜] ê¸°êµ¬ ì‹œì‘ì¼ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
        return pd.NaT


def sort_all_results_by_mech_start(all_results, sheets_service):
    def extract_start_date(entry):
        spreadsheet_url = entry[7]
        return get_mech_start_date(spreadsheet_url, sheets_service)

    return sorted(all_results, key=extract_start_date)


def collect_and_process_data():
    limit = int(os.getenv("LIMIT", "1"))
    batch_size = 10
    if not linked_spreadsheet_ids:
        print("ğŸš¨ [ì˜¤ë¥˜] ì¶”ì¶œëœ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ IDê°€ ì—†ìŠµë‹ˆë‹¤.")
        return []
    target_ids = linked_spreadsheet_ids[:limit] if limit > 0 else linked_spreadsheet_ids
    print(
        f"ì´ {len(linked_spreadsheet_ids)}ê°œ ì¤‘ ì²˜ìŒ {len(target_ids)}ê°œë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤."
    )
    sheet_values = fetch_entire_sheet_values(
        spreadsheet_id, f"'{TARGET_SHEET_NAME}'!A:AA"
    )
    all_results = []
    current_weekday = datetime.today().weekday()

    # ê·¸ë˜í”„ ìƒì„± ì˜µì…˜ í™•ì¸ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì œì–´)
    GENERATE_GRAPHS = os.getenv("GENERATE_GRAPHS", "auto").lower()

    if GENERATE_GRAPHS == "true":
        # ê°•ì œë¡œ ê·¸ë˜í”„ ìƒì„±
        generate_graphs_today = True
        print(
            "âœ… GENERATE_GRAPHS=true: ê°•ì œë¡œ ê·¸ë˜í”„ ìƒì„± ë° ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì§„í–‰"
        )
    elif GENERATE_GRAPHS == "false":
        # ê·¸ë˜í”„ ìƒì„± ë¹„í™œì„±í™”
        generate_graphs_today = False
        print("â›” GENERATE_GRAPHS=false: ê·¸ë˜í”„ ìƒì„± ë° ì—…ë°ì´íŠ¸ ë¹„í™œì„±í™”")
    else:
        # ìë™ ëª¨ë“œ: ì›”ìš”ì¼(0), ê¸ˆìš”ì¼(4)ì—ë§Œ ê·¸ë˜í”„ ìƒì„± - ë˜ëŠ” TEST_MODEì—ì„œëŠ” í•­ìƒ ìƒì„±
        generate_graphs_today = current_weekday in [0, 4] or TEST_MODE
    if generate_graphs_today:
        if TEST_MODE:
            print("âœ… TEST_MODE: ê·¸ë˜í”„ ìƒì„± ë° ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì§„í–‰")
        else:
            print("âœ… ì˜¤ëŠ˜ì€ ì›”ìš”ì¼/ê¸ˆìš”ì¼: ê·¸ë˜í”„ ìƒì„± ë° ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—…ë°ì´íŠ¸ ì§„í–‰")
    else:
        print(
            "â›” ì˜¤ëŠ˜ì€ ì›”ìš”ì¼/ê¸ˆìš”ì¼ì´ ì•„ë‹ˆë¯€ë¡œ ê·¸ë˜í”„ ìƒì„± ë° ì—…ë°ì´íŠ¸ëŠ” ìƒëµë©ë‹ˆë‹¤."
        )

    print(
        f"ğŸ“Š ê·¸ë˜í”„ ìƒì„± ì„¤ì •: GENERATE_GRAPHS={GENERATE_GRAPHS}, ì‹¤ì œ ìƒì„± ì—¬ë¶€: {generate_graphs_today}"
    )

    def process_batch(batch_ids):
        import time

        for idx, target_spreadsheet_id in enumerate(batch_ids, 1):
            try:
                print(f"--- ğŸš€ ì²˜ë¦¬ ì¤‘: {idx}/{len(batch_ids)} (Batch) ---")

                # Rate Limit ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—° (ì²« ë²ˆì§¸ê°€ ì•„ë‹Œ ê²½ìš°)
                if idx > 1:
                    print("â±ï¸ Rate Limit ë°©ì§€ë¥¼ ìœ„í•´ 3ì´ˆ ëŒ€ê¸°...")
                    time.sleep(3)

                df = fetch_data_from_sheets(target_spreadsheet_id, WORKSHEET_RANGE)
                product_name, mech_partner, elec_partner = fetch_info_board_extended(
                    target_spreadsheet_id
                )
                print(f"ğŸ“Œ Processing Model: {product_name}")
                task_total_time = process_data(df, product_name)
                task_total_time["ì‘ì—… ë¶„ë¥˜"] = task_total_time["ë‚´ìš©"].apply(
                    lambda x: classify_task(x, product_name)
                )
                order_no = get_spreadsheet_title(target_spreadsheet_id)
                print(f"ğŸ“Œ Processing Order No: {order_no}")
                spreadsheet_url = f"https://docs.google.com/spreadsheets/d/{target_spreadsheet_id}/edit"
                update_spreadsheet_with_product_name(
                    spreadsheet_id, order_no, product_name, sheet_values
                )
                if generate_graphs_today:
                    # ê·¸ë˜í”„ íŒŒì¼ë“¤ì„ ë¨¼ì € ìƒì„±
                    working_hours_file = generate_and_save_graph(
                        task_total_time, order_no, product_name
                    )
                    legend_file = generate_legend_chart(
                        task_total_time, order_no, product_name
                    )
                    wd_file = generate_and_save_graph_wd(
                        task_total_time, df, order_no, product_name
                    )

                    # Driveì— ì—…ë¡œë“œí•˜ê³  ë§í¬ ì—…ë°ì´íŠ¸
                    links = {
                        "working_hours": update_spreadsheet_with_working_hours(
                            spreadsheet_id,
                            order_no,
                            upload_to_drive(working_hours_file),
                            sheet_values,
                        ),
                        "legend": update_spreadsheet_with_legend(
                            spreadsheet_id,
                            order_no,
                            upload_to_drive(legend_file),
                            sheet_values,
                        ),
                        "wd": update_spreadsheet_with_wd_graph(
                            spreadsheet_id,
                            order_no,
                            upload_to_drive(wd_file),
                            sheet_values,
                        ),
                    }

                    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                    for temp_file in [working_hours_file, legend_file, wd_file]:
                        try:
                            if temp_file and os.path.exists(temp_file):
                                os.remove(temp_file)
                                logger.info(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ: {temp_file}")
                        except Exception as e:
                            logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ {temp_file}: {e}")
                else:
                    links = {"working_hours": None, "legend": None, "wd": None}
                    print("â›” ê·¸ë˜í”„ ìƒì„± ë° ë§í¬ ì—…ë°ì´íŠ¸ ìƒëµë¨")
                progress_summary = calculate_progress_by_category(df, product_name)
                total_time_decimal = task_total_time["ì›Œí‚¹ë°ì´ ì†Œìš” ì‹œê°„"].sum()
                total_time_formatted = format_hours(total_time_decimal)
                update_spreadsheet_with_total_time(
                    spreadsheet_id, order_no, total_time_formatted, sheet_values
                )
                print(
                    f"ğŸ¯ ì´ ì†Œìš”ì‹œê°„ {total_time_formatted}ì´ Wì—´ì— ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤."
                )
                mechanical_time_decimal = task_total_time[
                    task_total_time["ì‘ì—… ë¶„ë¥˜"] == "ê¸°êµ¬"
                ]["ì›Œí‚¹ë°ì´ ì†Œìš” ì‹œê°„"].sum()
                electrical_time_decimal = task_total_time[
                    task_total_time["ì‘ì—… ë¶„ë¥˜"] == "ì „ì¥"
                ]["ì›Œí‚¹ë°ì´ ì†Œìš” ì‹œê°„"].sum()
                inspection_time_decimal = task_total_time[
                    task_total_time["ì‘ì—… ë¶„ë¥˜"] == "ê²€ì‚¬"
                ]["ì›Œí‚¹ë°ì´ ì†Œìš” ì‹œê°„"].sum()
                finishing_time_decimal = task_total_time[
                    task_total_time["ì‘ì—… ë¶„ë¥˜"] == "ë§ˆë¬´ë¦¬"
                ]["ì›Œí‚¹ë°ì´ ì†Œìš” ì‹œê°„"].sum()
                update_spreadsheet_with_mechanical_time(
                    spreadsheet_id,
                    order_no,
                    format_hours(mechanical_time_decimal),
                    sheet_values,
                )
                update_spreadsheet_with_electrical_time(
                    spreadsheet_id,
                    order_no,
                    format_hours(electrical_time_decimal),
                    sheet_values,
                )
                update_spreadsheet_with_inspection_time(
                    spreadsheet_id,
                    order_no,
                    format_hours(inspection_time_decimal),
                    sheet_values,
                )
                update_spreadsheet_with_finishing_time(
                    spreadsheet_id,
                    order_no,
                    format_hours(finishing_time_decimal),
                    sheet_values,
                )
                print(f"ğŸ¯ ëª¨ë¸ '{order_no}'ì˜ ì‘ì—…ë³„ ì†Œìš”ì‹œê°„ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
                avg_mapping = get_avg_time_mapping(product_name)
                occurrence_stats, partner_stats = compute_occurrence_rates(
                    df,
                    task_total_time,
                    avg_mapping,
                    product_name,
                    tolerance=2,
                    mech_partner=mech_partner,
                    elec_partner=elec_partner,
                )
                if any(
                    stats["nan_count"] > 0 or stats["ot_count"] > 0
                    for stats in occurrence_stats.values()
                ):
                    all_results.append(
                        (
                            order_no,
                            product_name,
                            mech_partner,
                            elec_partner,
                            occurrence_stats,
                            partner_stats,
                            links,
                            spreadsheet_url,
                            progress_summary,
                        )
                    )
                else:
                    print("âœ… [ì•Œë¦¼] ëª¨ë“  ì‘ì—…ì´ ì •ìƒ ë²”ìœ„ ë‚´ì— ìˆìŠµë‹ˆë‹¤.")
                print(f"âœ… ëª¨ë¸ '{order_no}' ì²˜ë¦¬ ì™„ë£Œ.\n")
                systime.sleep(1)
            except Exception as e:
                print(
                    f"âŒ [ì˜¤ë¥˜ ë°œìƒ: ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ID {target_spreadsheet_id}] -> {e}\n"
                )
                systime.sleep(5)

    iterator = iter(target_ids)
    while batch := list(islice(iterator, batch_size)):
        process_batch(batch)
        systime.sleep(10)
    return all_results


def save_results_to_json(all_results, drive_service):
    """
    ì£¼ìš” ì²˜ë¦¬ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ê³  'JSON ë°ì´í„° ì €ì¥ìš©' êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    (ë°ì´í„° -> JSON)
    ì—…ë¡œë“œëœ íŒŒì¼ì˜ Google Drive IDë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not all_results:
        print("â„¹ï¸ ì²˜ë¦¬í•  ê²°ê³¼ê°€ ì—†ì–´ JSON íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None

    now_kst = datetime.now(pytz.timezone("Asia/Seoul"))
    execution_time_str = now_kst.strftime("%Y%m%d_%H%M%S")
    execution_time_for_json = execution_time_str  # ê¸°ì¡´ í˜•ì‹ ìœ ì§€: "20250618_231207"
    weekday_kor = ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"][now_kst.weekday()]
    session = now_kst.weekday() + 1
    filename = (
        f"output/nan_ot_results_{execution_time_str}_{weekday_kor}_{session}íšŒì°¨.json"
    )
    os.makedirs("output", exist_ok=True)

    results_list = []
    for res in all_results:
        (
            order_no,
            model_name,
            mech_partner,
            elec_partner,
            occurrence_stats,
            partner_stats,
            links,
            spreadsheet_url,
            progress_summary,
        ) = res

        total_tasks = sum(
            stats.get("total_count", 0) for stats in occurrence_stats.values()
        )

        def calc_ratio(cat_stats):
            total_count = cat_stats.get("total_count", 0)
            if total_count == 0:
                return 0.0, 0.0
            nan_ratio = (cat_stats.get("nan_count", 0) / total_count) * 100
            ot_ratio = (cat_stats.get("ot_count", 0) / total_count) * 100
            return nan_ratio, ot_ratio

        ratios = {}
        if "ê¸°êµ¬" in occurrence_stats:
            ratios["mech_nan_ratio"], ratios["mech_ot_ratio"] = calc_ratio(
                occurrence_stats["ê¸°êµ¬"]
            )
        if "ì „ì¥" in occurrence_stats:
            ratios["elec_nan_ratio"], ratios["elec_ot_ratio"] = calc_ratio(
                occurrence_stats["ì „ì¥"]
            )
        if "TMS_ë°˜ì œí’ˆ" in occurrence_stats:
            ratios["tms_nan_ratio"], ratios["tms_ot_ratio"] = calc_ratio(
                occurrence_stats["TMS_ë°˜ì œí’ˆ"]
            )

        # ê¸°ì¡´ JSON êµ¬ì¡°ì— ë§ê²Œ occurrence_statsì—ì„œ ì„¸ë¶€ ì •ë³´ ì œê±°
        cleaned_occurrence_stats = {}
        for category, stats in occurrence_stats.items():
            cleaned_occurrence_stats[category] = {
                "total_count": stats.get("total_count", 0),
                "nan_count": stats.get("nan_count", 0),
                "ot_count": stats.get("ot_count", 0),
            }

        # ê¸°ì¡´ JSON êµ¬ì¡°ì— ë§ê²Œ linksë¥¼ order_href í•˜ë‚˜ë¡œ í†µí•©
        legacy_links = {"order_href": spreadsheet_url}

        results_list.append(
            {
                "order_no": order_no,
                "model_name": model_name,
                "mech_partner": mech_partner,
                "elec_partner": elec_partner,
                "total_tasks": total_tasks,
                "ratios": ratios,
                "links": legacy_links,  # ê¸°ì¡´ êµ¬ì¡° ì‚¬ìš©
                "occurrence_stats": cleaned_occurrence_stats,  # ì„¸ë¶€ ì •ë³´ ì œê±°
                "partner_stats": partner_stats,
                "spreadsheet_url": "",  # ê¸°ì¡´ êµ¬ì¡°ì—ì„œëŠ” ë¹ˆ ë¬¸ìì—´
                # progress_summary ì œê±° (ê¸°ì¡´ êµ¬ì¡°ì— ì—†ìŒ)
            }
        )

    json_data = {"execution_time": execution_time_for_json, "results": results_list}

    with open(filename, "w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2, default=str)
    print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {filename}")

    # ì—…ë¡œë“œ ì‹œ JSON ì „ìš© í´ë” ID ì‚¬ìš©
    file_metadata = {
        "name": os.path.basename(filename),
        "parents": [JSON_DRIVE_FOLDER_ID],
    }
    media = MediaFileUpload(filename, mimetype="application/json")
    uploaded = (
        drive_service.files()
        .create(body=file_metadata, media_body=media, fields="id, name")
        .execute()
    )
    print(
        f"âœ… JSON êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ ì™„ë£Œ: {uploaded.get('name')} (id: {uploaded.get('id')})"
    )

    return uploaded.get("id")


def load_json_files_from_drive(
    drive_service, period="weekly", week_number=None, target_day=None
):
    """
    Google Drive í´ë” ë‚´ JSON íŒŒì¼ ë¡œë“œ
    period: "weekly" (ì£¼ê°„), "monthly" (ì›”ê°„)
    week_number: íŠ¹ì • ì£¼ì°¨ í•„í„°ë§ (ì£¼ê°„ìš©)
    target_day: íŠ¹ì • ìš”ì¼ íŒŒì¼ë§Œ ë¡œë“œ ("friday", "sunday", None=ëª¨ë“  ìš”ì¼, "mixed"=ì£¼ì°¨ë³„ í˜¼í•©)
    """

    # ì›”ê°„ íˆíŠ¸ë§µìš© ìŠ¤ë§ˆíŠ¸ target_day ìë™ ì„¤ì • (íš¨ìœ¨ì„± ê°œì„ )
    if period == "monthly" and target_day is None:
        target_day = "mixed"  # 32ì£¼ ì´ì „=ê¸ˆìš”ì¼, 33ì£¼ ì´í›„=ì¼ìš”ì¼ í˜¼í•©
        print("ğŸ“Š ì›”ê°„ íˆíŠ¸ë§µ: 32ì£¼ ì´ì „=ê¸ˆìš”ì¼, 33ì£¼ ì´í›„=ì¼ìš”ì¼ JSON í˜¼í•© ë¡œë“œ")

    query = f"'{JSON_DRIVE_FOLDER_ID}' in parents and name contains 'nan_ot_results_'"
    if target_day == "friday":
        query += " and name contains '_ê¸ˆ_'"
    elif target_day == "sunday":
        query += " and name contains '_ì¼_'"
    elif target_day == "mixed":
        # ê¸ˆìš”ì¼ê³¼ ì¼ìš”ì¼ ëª¨ë‘ í¬í•¨ (ë‚˜ì¤‘ì— ì£¼ì°¨ë³„ë¡œ í•„í„°ë§)
        query += " and (name contains '_ê¸ˆ_' or name contains '_ì¼_')"
    # target_dayê°€ Noneì´ë©´ ëª¨ë“  ìš”ì¼ í¬í•¨

    # ìµœëŒ€ 3ë²ˆê¹Œì§€ ì¬ì‹œë„ (Drive íŒŒì¼ ì²˜ë¦¬ ì§€ì—° ëŒ€ì‘)
    for attempt in range(3):
        files = (
            drive_service.files()
            .list(q=query, fields="files(id, name)")
            .execute()
            .get("files", [])
        )

        if files:
            break

        if attempt < 2:  # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ëŒ€ê¸°
            print(f"â³ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 3ì´ˆ í›„ ì¬ì‹œë„... ({attempt + 1}/3)")
            systime.sleep(3)
        else:
            print("âš ï¸ ë¡œë“œí•  JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []

    data_list = []

    for file in files:
        file_name = file["name"]
        match = re.search(r"nan_ot_results_(\d{8})", file_name)
        if not match:
            continue

        file_date = match.group(1)  # YYYYMMDD
        try:
            file_datetime = pd.to_datetime(file_date, format="%Y%m%d")
            file_week = file_datetime.isocalendar().week
        except ValueError:
            continue

        # week_number í•„í„°ë§
        if week_number is not None and file_week != week_number:
            continue

        # mixed ëª¨ë“œì—ì„œ ì£¼ì°¨ë³„ ìš”ì¼ í•„í„°ë§ (32ì£¼ ì´ì „=ê¸ˆìš”ì¼, 33ì£¼ ì´í›„=ì¼ìš”ì¼)
        if target_day == "mixed":
            if file_week < 33:
                # 32ì£¼ ì´ì „: ê¸ˆìš”ì¼ë§Œ
                if "_ê¸ˆ_" not in file_name:
                    continue
            else:
                # 33ì£¼ ì´í›„: ì¼ìš”ì¼ë§Œ
                if "_ì¼_" not in file_name:
                    continue

        print(f"ğŸ“ JSON íŒŒì¼ ë¡œë“œ ì¤‘: {file_name}")
        file_id = file["id"]
        request = drive_service.files().get_media(fileId=file_id)
        content = request.execute().decode("utf-8")
        data = json.loads(content)
        for result in data["results"]:
            result["execution_time"] = data["execution_time"]
        data_list.extend(data["results"])

    print(f"ğŸ“‚ ì´ {len(data_list)}ê°œì˜ ë¡œê·¸ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    return data_list


def ratio_calc(stats):
    """NaN ë¹„ìœ¨ ê³„ì‚°"""
    total = stats.get("total_count", 0)
    nan_count = stats.get("nan_count", 0)
    return (nan_count / total * 100) if total > 0 else 0


def generate_heatmap(
    drive_service,
    period="weekly",
    group_by="partner",
    week_number=None,
    target_day=None,
):
    """
    íˆíŠ¸ë§µ ìƒì„± í•¨ìˆ˜
    period: "weekly" (ì£¼ê°„), "monthly" (ì›”ê°„)
    group_by: "partner" (í˜‘ë ¥ì‚¬), "model" (ëª¨ë¸)
    target_day: ì›”ê°„ íˆíŠ¸ë§µìš© íŠ¹ì • ìš”ì¼ ("friday", "sunday", None=auto)
    """
    # í°íŠ¸ ì„¤ì • ì¶”ê°€
    global font_prop
    try:
        # font_propê°€ ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        font_prop
    except NameError:
        # font_propê°€ ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
        font_prop = None
    # ë°ì´í„° ë¡œë“œ (ì›”ê°„ íˆíŠ¸ë§µì˜ ê²½ìš° load_json_files_from_driveì—ì„œ ìŠ¤ë§ˆíŠ¸ target_day ìë™ ì„¤ì •)
    all_data = load_json_files_from_drive(
        drive_service, period, week_number, target_day
    )

    if not all_data:
        print("âš ï¸ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # DataFrame ìƒì„± ë° ë°ì´í„° ë³€í™˜
    df_data = []
    for d in all_data:
        execution_time = d["execution_time"]
        occurrence_stats = d.get("occurrence_stats", {})
        mech_partner = d.get("mech_partner", "").strip().upper()
        elec_partner = d.get("elec_partner", "").strip().upper()

        entry = {
            "date": execution_time,
            "model_name": d["model_name"],
            "mech_partner": mech_partner,
            "elec_partner": elec_partner,
            "bat_nan_ratio": 0.0,
            "fni_nan_ratio": 0.0,
            "tms_m_nan_ratio": 0.0,
            "cna_nan_ratio": 0.0,
            "pns_nan_ratio": 0.0,
            "tms_e_nan_ratio": 0.0,
            "tms_semi_nan_ratio": 0.0,
        }

        # ê¸°êµ¬ í˜‘ë ¥ì‚¬ë³„ NaN ë¹„ìœ¨ ê³„ì‚°
        if mech_partner == "BAT":
            target = occurrence_stats.get("ê¸°êµ¬", {})
            entry["bat_nan_ratio"] = ratio_calc(target)
        elif mech_partner == "FNI":
            target = occurrence_stats.get("ê¸°êµ¬", {})
            entry["fni_nan_ratio"] = ratio_calc(target)
        elif mech_partner == "TMS":
            target = occurrence_stats.get("ê¸°êµ¬", {})
            entry["tms_m_nan_ratio"] = ratio_calc(target)

        # ì „ì¥ í˜‘ë ¥ì‚¬ë³„ NaN ë¹„ìœ¨ ê³„ì‚°
        if elec_partner == "C&A":
            target = occurrence_stats.get("ì „ì¥", {})
            entry["cna_nan_ratio"] = ratio_calc(target)
        elif elec_partner == "P&S":
            target = occurrence_stats.get("ì „ì¥", {})
            entry["pns_nan_ratio"] = ratio_calc(target)
        elif elec_partner == "TMS":
            target = occurrence_stats.get("ì „ì¥", {})
            entry["tms_e_nan_ratio"] = ratio_calc(target)

        # TMS ë°˜ì œí’ˆ NaN ë¹„ìœ¨
        tms_semi_stats = occurrence_stats.get("TMS_ë°˜ì œí’ˆ", {})
        entry["tms_semi_nan_ratio"] = ratio_calc(tms_semi_stats)

        df_data.append(entry)

    df = pd.DataFrame(df_data)
    df["date"] = pd.to_datetime(df["date"], format="%Y%m%d_%H%M%S", errors="coerce")
    df = df.dropna(subset=["date"])

    # í˜‘ë ¥ì‚¬ ì¹´í…Œê³ ë¦¬ ì •ì˜
    partner_categories = [
        ("bat_nan_ratio", "BAT", "blue"),
        ("fni_nan_ratio", "FNI", "cyan"),
        ("tms_m_nan_ratio", "TMS(m)", "orange"),
        ("cna_nan_ratio", "C&A", "green"),
        ("pns_nan_ratio", "P&S", "red"),
        ("tms_e_nan_ratio", "TMS(e)", "purple"),
        ("tms_semi_nan_ratio", "TMS_ë°˜ì œí’ˆ", "magenta"),
    ]

    # ì£¼ê°„/ì›”ê°„ë³„ ê·¸ë£¹í•‘
    if period == "weekly":
        df = df.sort_values("date")
        df["day"] = df["date"].dt.strftime("%mì›”%dì¼")  # ë‚ ì§œ í˜•ì‹ ë³€ê²½

        if group_by == "partner":
            df_grouped = df.groupby("day").mean(numeric_only=True)
            categories = partner_categories
            labels = list(df_grouped.index)  # ì´ë¯¸ í¬ë§·ëœ ë‚ ì§œ ì‚¬ìš©
            title = "ì£¼ê°„ NaN ë¹„ìœ¨ ì¶”ì´ (mixed)"  # ê¸°ì¡´ ì œëª©ê³¼ ì¼ì¹˜
            y_label = "í˜‘ë ¥ì‚¬"
        elif group_by == "model":
            # ì£¼ê°„ ëª¨ë¸ë³„ ë°ì´í„° ì²˜ë¦¬
            model_daily_data = []
            for (day, model), group in df.groupby(["day", "model_name"]):
                total_nan_ratio = 0
                count = 0
                for col, _, _ in partner_categories:
                    if col in group.columns and group[col].sum() > 0:
                        total_nan_ratio += group[col].mean()
                        count += 1
                avg_nan_ratio = total_nan_ratio / count if count > 0 else 0
                model_daily_data.append(
                    {"day": day, "model_name": model, "avg_nan_ratio": avg_nan_ratio}
                )

            model_df = pd.DataFrame(model_daily_data)
            if not model_df.empty:
                df_grouped = model_df.pivot(
                    index="model_name", columns="day", values="avg_nan_ratio"
                ).fillna(0)
                labels = [f"{d.month}ì›”{d.day}ì¼" for d in df_grouped.columns]
            else:
                print("âš ï¸ ëª¨ë¸ë³„ ì£¼ê°„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            title = "ì£¼ê°„ ëª¨ë¸ë³„ NaN ë¹„ìœ¨ íˆíŠ¸ë§µ"
            y_label = "ëª¨ë¸"

    elif period == "monthly":
        if group_by == "partner":
            df_grouped = df.groupby(df["date"].dt.to_period("M")).mean(
                numeric_only=True
            )
            df_grouped.index = df_grouped.index.to_timestamp()
            categories = partner_categories
            labels = [d.strftime("%Y-%m") for d in df_grouped.index]
            title = "ì›”ê°„ í˜‘ë ¥ì‚¬ë³„ NaN ë¹„ìœ¨ íˆíŠ¸ë§µ (ê¸ˆìš”ì¼ ê¸°ì¤€)"
            y_label = "í˜‘ë ¥ì‚¬"
        elif group_by == "model":
            # ê¸°ì¡´ ë°©ì‹ê³¼ ì™„ì „íˆ ë™ì¼: ì›”ê°„ ëª¨ë¸ë³„ ì²˜ë¦¬
            df_grouped = (
                df.groupby([df["date"].dt.to_period("M"), "model_name"])
                .mean(numeric_only=True)
                .reset_index()
            )
            df_grouped["date"] = df_grouped["date"].apply(lambda x: x.to_timestamp())

            # ëª¨ë¸ëª… ë¦¬ìŠ¤íŠ¸ ìƒì„± (categories ë³€ìˆ˜)
            categories = [
                (row["model_name"], row["model_name"], "blue")
                for _, row in df_grouped[["model_name"]].drop_duplicates().iterrows()
            ]

            # Pivot í…Œì´ë¸” ìƒì„±: ëª¨ë“  í˜‘ë ¥ì‚¬ ë¹„ìœ¨ ì»¬ëŸ¼ ìœ ì§€
            df_grouped = df_grouped.pivot(
                index="date",
                columns="model_name",
                values=[col[0] for col in partner_categories],
            )

            # í•µì‹¬! ê¸°ì¡´ ë°©ì‹: ì»¬ëŸ¼ëª… ë‹¨ìˆœí™”
            df_grouped.columns = [col[1] for col in df_grouped.columns]

            labels = [d.strftime("%Y-%m") for d in df_grouped.index]
            title = "ì›”ê°„ NaN ë¹„ìœ¨ ì¶”ì´ (ê¸ˆìš”ì¼ ê¸°ì¤€)"
            y_label = "ëª¨ë¸"

    # íˆíŠ¸ë§µ ë°ì´í„° ì¤€ë¹„
    if group_by == "partner":
        heatmap_data = df_grouped[[cat[0] for cat in partner_categories]].T
        heatmap_data.index = [cat[1] for cat in partner_categories]
    else:
        # ëª¨ë¸ë³„ íˆíŠ¸ë§µ: ê¸°ì¡´ ë°©ì‹ê³¼ ë™ì¼í•˜ê²Œ groupbyë¡œ í‰ê·  ê³„ì‚° í›„ transpose
        heatmap_data = df_grouped.groupby(axis=1, level=0).mean().T

    # íˆíŠ¸ë§µ ìƒì„±
    plt.figure(figsize=(12, max(6, len(heatmap_data.index) * 0.6)))
    sns.heatmap(
        heatmap_data,
        annot=True,
        fmt=".1f",
        cmap="YlOrRd",
        cbar_kws={"label": "NaN ë¹„ìœ¨ (%)"},
        linewidths=0.5,
    )

    if font_prop:
        plt.title(title, fontproperties=font_prop, fontsize=16)
        plt.xlabel("ì¸¡ì • ë‚ ì§œ", fontproperties=font_prop)
        plt.ylabel(y_label, fontproperties=font_prop)
        plt.xticks(
            ticks=np.arange(len(labels)) + 0.5,
            labels=labels,
            rotation=45,
            ha="right",
            fontproperties=font_prop,
        )
        plt.yticks(rotation=0, fontproperties=font_prop)
    else:
        plt.title(title, fontsize=16)
        plt.xlabel("ì¸¡ì • ë‚ ì§œ")
        plt.ylabel(y_label)
        plt.xticks(
            ticks=np.arange(len(labels)) + 0.5, labels=labels, rotation=45, ha="right"
        )
        plt.yticks(rotation=0)
    plt.tight_layout()

    # íŒŒì¼ëª… ë° ì €ì¥
    filename = f"output/{period}_{group_by}_nan_heatmap_{datetime.now().strftime('%Y%m%d')}.png"
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    plt.savefig(filename, bbox_inches="tight")
    plt.close()

    print(f"âœ… {title} ìƒì„± ì™„ë£Œ: {filename}")
    return filename


def generate_weekly_report_heatmap(drive_service, output_path=None):
    """
    ì´ë²ˆ ì£¼(ì›”~ê¸ˆ)ì˜ ëª¨ë“  JSONì„ Driveì—ì„œ ì½ì–´, í˜‘ë ¥ì‚¬ë³„/ë‚ ì§œë³„ NaN ë¹„ìœ¨ íˆíŠ¸ë§µì„ ìƒì„±í•©ë‹ˆë‹¤.
    (Drive JSONs -> Weekly Heatmap)
    """
    # í°íŠ¸ ì„¤ì • ì¶”ê°€
    global font_prop
    try:
        # font_propê°€ ì •ì˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        font_prop
    except NameError:
        # font_propê°€ ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
        font_prop = None
    # 1. ì´ë²ˆ ì£¼ ë‚ ì§œ ë° ì£¼ì°¨ ê³„ì‚° (ì›”ìš”ì¼ ~ ê¸ˆìš”ì¼)
    today = datetime.now(pytz.timezone("Asia/Seoul"))
    start_of_week = today - timedelta(days=today.weekday())
    end_of_week = start_of_week + timedelta(days=4)
    current_week = today.isocalendar().week
    print(
        f"\n--- ğŸ“Š ì£¼ê°„ íˆíŠ¸ë§µ ìƒì„±ì„ ìœ„í•´ {current_week}ì£¼ì°¨ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤ ---"
    )
    print(
        f"({start_of_week.strftime('%Y-%m-%d')} ~ {end_of_week.strftime('%Y-%m-%d')})"
    )

    # 2. ì´ë²ˆ ì£¼ì°¨ ë°ì´í„°ë§Œ ë¡œë“œ (íš¨ìœ¨ì„± ê°œì„ )
    all_data = load_json_files_from_drive(
        drive_service, period="weekly", week_number=current_week
    )

    if not all_data:
        print("âš ï¸ ì´ë²ˆ ì£¼ ë°ì´í„°ê°€ ì—†ì–´ ì£¼ê°„ íˆíŠ¸ë§µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # 3. DataFrame ìƒì„± ë° ë°ì´í„° ë³€í™˜
    df_data = []
    for d in all_data:
        execution_time = d["execution_time"]
        occurrence_stats = d.get("occurrence_stats", {})
        mech_partner = d.get("mech_partner", "").strip().upper()
        elec_partner = d.get("elec_partner", "").strip().upper()

        # ë‚ ì§œ íŒŒì‹±
        try:
            if isinstance(execution_time, str):
                if "_" in execution_time:  # ê¸°ì¡´ í˜•ì‹: 20250616_132845
                    date_obj = pd.to_datetime(execution_time, format="%Y%m%d_%H%M%S")
                else:  # ìƒˆ í˜•ì‹: 2025-06-18 23:12:07
                    date_obj = pd.to_datetime(execution_time)
            else:
                date_obj = pd.to_datetime(execution_time)
        except:
            continue

        # ì´ë²ˆ ì£¼ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œ í•„í„°ë§
        if not (start_of_week.date() <= date_obj.date() <= end_of_week.date()):
            continue

        entry = {
            "date": date_obj,
            "model_name": d["model_name"],
            "mech_partner": mech_partner,
            "elec_partner": elec_partner,
            "bat_nan_ratio": 0.0,
            "fni_nan_ratio": 0.0,
            "tms_m_nan_ratio": 0.0,
            "cna_nan_ratio": 0.0,
            "pns_nan_ratio": 0.0,
            "tms_e_nan_ratio": 0.0,
            "tms_semi_nan_ratio": 0.0,
        }

        # ê¸°êµ¬ í˜‘ë ¥ì‚¬ë³„ NaN ë¹„ìœ¨ ê³„ì‚°
        if mech_partner == "BAT":
            target = occurrence_stats.get("ê¸°êµ¬", {})
            entry["bat_nan_ratio"] = ratio_calc(target)
        elif mech_partner == "FNI":
            target = occurrence_stats.get("ê¸°êµ¬", {})
            entry["fni_nan_ratio"] = ratio_calc(target)
        elif mech_partner == "TMS":
            target = occurrence_stats.get("ê¸°êµ¬", {})
            entry["tms_m_nan_ratio"] = ratio_calc(target)

        # ì „ì¥ í˜‘ë ¥ì‚¬ë³„ NaN ë¹„ìœ¨ ê³„ì‚°
        if elec_partner == "C&A":
            target = occurrence_stats.get("ì „ì¥", {})
            entry["cna_nan_ratio"] = ratio_calc(target)
        elif elec_partner == "P&S":
            target = occurrence_stats.get("ì „ì¥", {})
            entry["pns_nan_ratio"] = ratio_calc(target)
        elif elec_partner == "TMS":
            target = occurrence_stats.get("ì „ì¥", {})
            entry["tms_e_nan_ratio"] = ratio_calc(target)

        # TMS ë°˜ì œí’ˆ NaN ë¹„ìœ¨
        tms_semi_stats = occurrence_stats.get("TMS_ë°˜ì œí’ˆ", {})
        entry["tms_semi_nan_ratio"] = ratio_calc(tms_semi_stats)

        df_data.append(entry)

    if not df_data:
        print("âš ï¸ ì´ë²ˆ ì£¼ ë°ì´í„°ê°€ ì—†ì–´ ì£¼ê°„ íˆíŠ¸ë§µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    df = pd.DataFrame(df_data)
    df = df.sort_values("date")
    df["day"] = df["date"].dt.strftime("%mì›”%dì¼")

    # í˜‘ë ¥ì‚¬ ì¹´í…Œê³ ë¦¬ ì •ì˜
    partner_categories = [
        ("bat_nan_ratio", "BAT"),
        ("fni_nan_ratio", "FNI"),
        ("tms_m_nan_ratio", "TMS(m)"),
        ("cna_nan_ratio", "C&A"),
        ("pns_nan_ratio", "P&S"),
        ("tms_e_nan_ratio", "TMS(e)"),
        ("tms_semi_nan_ratio", "TMS_ë°˜ì œí’ˆ"),
    ]

    # ë‚ ì§œë³„ ê·¸ë£¹í™” ë° í‰ê·  ê³„ì‚°
    df_grouped = df.groupby("day").mean(numeric_only=True)

    if df_grouped.empty:
        print("âš ï¸ ê·¸ë£¹í™”ëœ ë°ì´í„°ê°€ ì—†ì–´ íˆíŠ¸ë§µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # íˆíŠ¸ë§µ ë°ì´í„° ì¤€ë¹„ (í˜‘ë ¥ì‚¬ x ë‚ ì§œ)
    heatmap_data = []
    for col, label in partner_categories:
        if col in df_grouped.columns:
            heatmap_data.append(df_grouped[col].values)
        else:
            heatmap_data.append([0] * len(df_grouped))

    heatmap_array = np.array(heatmap_data)
    partner_labels = [label for _, label in partner_categories]
    date_labels = list(df_grouped.index)

    # íˆíŠ¸ë§µ ìƒì„±
    plt.figure(
        figsize=(max(8, len(date_labels) * 1.5), max(6, len(partner_labels) * 0.8))
    )
    sns.heatmap(
        heatmap_array,
        annot=True,
        fmt=".1f",
        cmap="Reds",
        linewidths=0.5,
        xticklabels=date_labels,
        yticklabels=partner_labels,
        cbar_kws={"label": "NaN ë¹„ìœ¨ (%)"},
    )

    if font_prop:
        plt.title("ì£¼ê°„ NaN ë¹„ìœ¨ ì¶”ì´ (mixed)", fontproperties=font_prop, fontsize=16)
        plt.xlabel("ì¸¡ì • ë‚ ì§œ", fontproperties=font_prop)
        plt.ylabel("í˜‘ë ¥ì‚¬", fontproperties=font_prop)
        plt.xticks(rotation=0, fontproperties=font_prop)
        plt.yticks(rotation=0, fontproperties=font_prop)
    else:
        plt.title("ì£¼ê°„ NaN ë¹„ìœ¨ ì¶”ì´ (mixed)", fontsize=16)
        plt.xlabel("ì¸¡ì • ë‚ ì§œ")
        plt.ylabel("í˜‘ë ¥ì‚¬")
        plt.xticks(rotation=0)
        plt.yticks(rotation=0)
    plt.tight_layout()

    # ë‚ ì§œê°€ í¬í•¨ëœ íŒŒì¼ëª… ìƒì„±
    if output_path is None:
        output_path = (
            f"output/weekly_partner_nan_heatmap_{datetime.now().strftime('%Y%m%d')}.png"
        )

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    plt.savefig(output_path, bbox_inches="tight")
    plt.close()

    print(f"âœ… ì£¼ê°„ ë¦¬í¬íŠ¸ìš© íˆíŠ¸ë§µ ì €ì¥ ì™„ë£Œ: {output_path}")
    return output_path


def should_generate_monthly_heatmap():
    """ì›”ê°„ íˆíŠ¸ë§µ ìƒì„± ì¡°ê±´ í™•ì¸ (33ì£¼ë¶€í„° ì¼ìš”ì¼ë¡œ ë³€ê²½)"""
    today = datetime.now(pytz.timezone("Asia/Seoul"))
    current_week = today.isocalendar().week

    if current_week < 33:
        # 32ì£¼ ì´ì „: ê¸ˆìš”ì¼ ê¸°ì¤€
        if today.weekday() != 4:  # ê¸ˆìš”ì¼ì´ ì•„ë‹ˆë©´
            return False, "friday"

        # ë‹¤ìŒ ì£¼ ê¸ˆìš”ì¼ì´ ë‹¤ìŒ ë‹¬ì¸ì§€ í™•ì¸
        next_friday = today + timedelta(days=7)
        return next_friday.month != today.month, "friday"
    else:
        # 33ì£¼ë¶€í„°: ì¼ìš”ì¼ ê¸°ì¤€
        if today.weekday() != 6:  # ì¼ìš”ì¼ì´ ì•„ë‹ˆë©´
            return False, "sunday"

        # ë‹¤ìŒ ì£¼ ì¼ìš”ì¼ì´ ë‹¤ìŒ ë‹¬ì¸ì§€ í™•ì¸
        next_sunday = today + timedelta(days=7)
        return next_sunday.month != today.month, "sunday"


def generate_final_html(
    all_results,
    heatmap_path,
    output_filename="partner.html",
    monthly_partner_link=None,
    monthly_model_link=None,
):
    """
    ì²˜ë¦¬ëœ ë°ì´í„°ì™€ íˆíŠ¸ë§µì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… HTML íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    heatmap_url_for_html = upload_to_drive(heatmap_path) if heatmap_path else None

    html_body = build_combined_email_body(
        all_results,
        heatmap_url=heatmap_url_for_html,
        monthly_partner_url=monthly_partner_link,
        monthly_model_url=monthly_model_link,
    )

    # generate_html_from_content í•¨ìˆ˜ê°€ íŒŒì¼ ì €ì¥ê³¼ ì—…ë¡œë“œë¥¼ ê°™ì´ í•˜ë¯€ë¡œ ë¶„ë¦¬í•  í•„ìš”ê°€ ìˆìŒ
    # ì§€ê¸ˆì€ ê¸°ì¡´ í•¨ìˆ˜ë¥¼ ì¬í™œìš©í•˜ì—¬ ê²½ë¡œë§Œ ë°˜í™˜
    styled_html = f"""
    <html>
    <head>
      <meta charset="UTF-8">
      <title>PDA Dashboard</title>
      <style>
        body {{ font-family: 'NanumGothic', sans-serif; font-size: 12px; margin: 20px; }}
        table {{ border-collapse: collapse; width: 100%; margin-bottom: 20px; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
        details {{ margin-bottom: 10px; }}
        summary {{ cursor: pointer; font-weight: bold; color: #333; }}
        details[open] summary {{ color: #0056b3; }}
        details > *:not(summary) {{ display: block; margin-left: 20px; }}
        ul {{ margin: 5px 0; padding-left: 20px; }}
        p {{ margin: 5px 0; }}
        hr {{ margin: 20px 0; }}
        a {{ color: #0056b3; text-decoration: none; }}
        a:hover {{ text-decoration: underline; }}
        img {{ max-width: 100%; height: auto; }}
      </style>
    </head>
    <body>{html_body}</body></html>
    """
    with open(output_filename, "w", encoding="utf-8") as f:
        f.write(styled_html)
    print(f"ğŸ“„ ìµœì¢… HTML íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_filename}")
    return output_filename


# ====================================
# MAIN EXECUTION BLOCK (REFACTORED)
# ====================================
if __name__ == "__main__":
    # 1. ë°ì´í„° ì¶”ì¶œ ë° ê°€ê³µ
    print("--- 1. ë°ì´í„° ì¶”ì¶œ ë° ê°€ê³µ ì‹œì‘ ---")
    all_results = collect_and_process_data()

    if all_results:
        # 2. ê²°ê³¼ JSONìœ¼ë¡œ ì €ì¥ ë° Drive ì—…ë¡œë“œ
        print("\n--- 2. ê²°ê³¼ JSONìœ¼ë¡œ ì €ì¥ ë° ì—…ë¡œë“œ ì‹œì‘ ---")
        save_results_to_json(all_results, drive_service)  # ID ë°˜í™˜ê°’ì€ ë”ì´ìƒ í•„ìš” ì—†ìŒ

        # JSON ì—…ë¡œë“œ í›„ Driveì—ì„œ íŒŒì¼ì´ ì™„ì „íˆ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
        print("â±ï¸ Google Drive íŒŒì¼ ì²˜ë¦¬ ëŒ€ê¸° ì¤‘... (5ì´ˆ)")
        systime.sleep(5)

        # 3. ì£¼ê°„/ì›”ê°„ íˆíŠ¸ë§µ ìƒì„±
        print("\n--- 3. íˆíŠ¸ë§µ ìƒì„± ì‹œì‘ ---")

        # 3-1. ì£¼ê°„ ë¦¬í¬íŠ¸ìš© íˆíŠ¸ë§µ (ì´ë²ˆ ì£¼ ëª¨ë“  JSON ì·¨í•©)
        heatmap_path = generate_weekly_report_heatmap(drive_service)

        # 3-2. ì›”ê°„ íˆíŠ¸ë§µ ìƒì„± (33ì£¼ë¶€í„° ì¼ìš”ì¼ ê¸°ì¤€ìœ¼ë¡œ ë³€ê²½)
        monthly_partner_link = None
        monthly_model_link = None

        should_generate, target_day = should_generate_monthly_heatmap()
        if should_generate:
            if target_day == "friday":
                print("\n--- ğŸ“Š ì›”ì˜ ë§ˆì§€ë§‰ ê¸ˆìš”ì¼: ì›”ê°„ íˆíŠ¸ë§µ ìƒì„± ì‹œì‘ ---")
            else:
                print("\n--- ğŸ“Š ì›”ì˜ ë§ˆì§€ë§‰ ì¼ìš”ì¼: ì›”ê°„ íˆíŠ¸ë§µ ìƒì„± ì‹œì‘ ---")

            # ì›”ê°„ í˜‘ë ¥ì‚¬ íˆíŠ¸ë§µ
            monthly_partner_heatmap = generate_heatmap(
                drive_service,
                period="monthly",
                group_by="partner",
                target_day=target_day,
            )

            # ì›”ê°„ ëª¨ë¸ íˆíŠ¸ë§µ
            monthly_model_heatmap = generate_heatmap(
                drive_service, period="monthly", group_by="model", target_day=target_day
            )

            print(f"âœ… ì›”ê°„ íˆíŠ¸ë§µ ìƒì„± ì™„ë£Œ:")
            if monthly_partner_heatmap:
                print(f"   - í˜‘ë ¥ì‚¬ë³„: {monthly_partner_heatmap}")
                # ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ
                monthly_partner_link = upload_to_drive(monthly_partner_heatmap)
                if monthly_partner_link:
                    print(f"   - í˜‘ë ¥ì‚¬ë³„ ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ ì™„ë£Œ: {monthly_partner_link}")
            if monthly_model_heatmap:
                print(f"   - ëª¨ë¸ë³„: {monthly_model_heatmap}")
                # ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ
                monthly_model_link = upload_to_drive(monthly_model_heatmap)
                if monthly_model_link:
                    print(f"   - ëª¨ë¸ë³„ ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ ì™„ë£Œ: {monthly_model_link}")
        else:
            current_week = datetime.now(pytz.timezone("Asia/Seoul")).isocalendar().week
            if current_week < 33:
                print(
                    "â„¹ï¸ ì›”ì˜ ë§ˆì§€ë§‰ ê¸ˆìš”ì¼ì´ ì•„ë‹ˆë¯€ë¡œ ì›”ê°„ íˆíŠ¸ë§µì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                )
            else:
                print(
                    "â„¹ï¸ ì›”ì˜ ë§ˆì§€ë§‰ ì¼ìš”ì¼ì´ ì•„ë‹ˆë¯€ë¡œ ì›”ê°„ íˆíŠ¸ë§µì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                )

        # 4. ìµœì¢… HTML ìƒì„±
        print("\n--- 4. ìµœì¢… HTML ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘ ---")
        # generate_nan_bar_chartsëŠ” all_resultsë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì—¬ê¸°ì„œ í˜¸ì¶œ
        tasks_file, total_file = generate_nan_bar_charts(all_results)
        final_html_path = generate_final_html(
            all_results,
            heatmap_path,
            output_filename="partner.html",
            monthly_partner_link=monthly_partner_link,
            monthly_model_link=monthly_model_link,
        )

        # 5. ì•Œë¦¼ ë° ì—…ë¡œë“œ
        print("\n--- 5. ì•Œë¦¼ ë° ì—…ë¡œë“œ ì‹œì‘ ---")

        # GitHub ì—…ë¡œë“œ ì˜µì…˜ í™•ì¸ (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì œì–´)
        GITHUB_UPLOAD = os.getenv("GITHUB_UPLOAD", "auto").lower()

        should_upload_github = False
        if GITHUB_UPLOAD == "true":
            should_upload_github = True
            print("âœ… GITHUB_UPLOAD=true: ê°•ì œë¡œ GitHub ì—…ë¡œë“œ ì§„í–‰")
        elif GITHUB_UPLOAD == "false":
            should_upload_github = False
            print("â›” GITHUB_UPLOAD=false: GitHub ì—…ë¡œë“œ ë¹„í™œì„±í™”")
        else:
            # ìë™ ëª¨ë“œ: TEST_MODEê°€ ì•„ë‹ ë•Œë§Œ ì—…ë¡œë“œ
            should_upload_github = not TEST_MODE
            if should_upload_github:
                print("âœ… ìš´ì˜ ëª¨ë“œ: GitHub ì—…ë¡œë“œ ì§„í–‰")
            else:
                print("ğŸ§ª TEST_MODE: GitHub ì—…ë¡œë“œ ìƒëµ")

        print(
            f"ğŸ“¤ GitHub ì—…ë¡œë“œ ì„¤ì •: GITHUB_UPLOAD={GITHUB_UPLOAD}, ì‹¤ì œ ì—…ë¡œë“œ ì—¬ë¶€: {should_upload_github}"
        )

        if should_upload_github:
            upload_to_github(final_html_path)
            print(f"âœ… GitHub ì—…ë¡œë“œ ì™„ë£Œ: {final_html_path}")
        else:
            print(f"â›” GitHub ì—…ë¡œë“œ ìƒëµë¨: {final_html_path}")

        # ì´ë©”ì¼ ë°œì†¡
        email_body = open(final_html_path, "r", encoding="utf-8").read()
        attachment_files = [f for f in [tasks_file, total_file, heatmap_path] if f]
        send_occurrence_email(
            f"[ì•Œë¦¼] PDA Overtime ë° NaN ì²´í¬ ê²°ê³¼ - ì´ {len(all_results)}ê±´",
            email_body,
            graph_files=attachment_files,
        )

        # ì¹´ì¹´ì˜¤í†¡ ë°œì†¡
        send_nan_alert_to_kakao(all_results)

        print("\nâœ… [ì¢…ë£Œ] ë°ì´í„° ì¤‘ì‹¬ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ì™„ë£Œ.")

    else:
        print("\nâœ… [ì¢…ë£Œ] ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ì–´ ì‘ì—…ì„ ë§ˆì¹©ë‹ˆë‹¤.")
