"""
PDA (Production Data Analysis) ì‹œìŠ¤í…œ ë©”ì¸ ì‹¤í–‰ íŒŒì¼
"""

import argparse
import json
import os
from datetime import datetime

import numpy as np
import pandas as pd
from src.calculate import TaskAnalyzer, WorkTimeCalculator, analyze_occurrence_rates
from src.notification import EmailNotifier, KakaoNotifier
from src.settings import (
    GITHUB_REPO,
    GITHUB_USERNAME,
    INFO_RANGE,
    PROCESS_LIMIT,
    WORKSHEET_RANGE,
    get_google_services,
    logger,
)
from src.sheets import SheetsProcessor
from src.upload import DriveUploader, GitHubUploader
from src.utils import get_file_paths, save_json
from src.visualization import HeatmapGenerator, ReportGenerator

from task_analyzer import TaskAnalyzer


def main(args):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™•ì¸
        if args.test_mode:
            logger.info("ğŸ”§ í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
            logger.info("âš ï¸ GitHub, Drive ì—…ë¡œë“œ ë° ì•Œë¦¼ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

        # Google ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        sheets_service, drive_service = get_google_services()
        sheets_processor = SheetsProcessor(sheets_service)

        # í•˜ì´í¼ë§í¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        target_sheet_name = os.getenv("TARGET_SHEET_NAME", "ì¶œí•˜ì˜ˆì •ë¦¬ìŠ¤íŠ¸(TEST)")
        hyperlinks = sheets_processor.get_hyperlinks(args.spreadsheet_id, f"'{target_sheet_name}'!A3:A")

        if not hyperlinks:
            logger.error("âŒ í•˜ì´í¼ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        logger.info(f"ğŸ“‹ ì²˜ë¦¬í•  ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ìˆ˜: {min(len(hyperlinks), PROCESS_LIMIT)}")

        # ë°ì´í„° ì²˜ë¦¬ ê²°ê³¼ ì €ì¥
        processed_data = []

        # ê° í•˜ì´í¼ë§í¬ ì²˜ë¦¬
        for i, hyperlink in enumerate(hyperlinks[:PROCESS_LIMIT]):
            # ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ID ì¶”ì¶œ
            spreadsheet_id = hyperlink["spreadsheet_id"]

            # ì •ë³´íŒ ì‹œíŠ¸ì—ì„œ ëª¨ë¸ëª…ê³¼ í˜‘ë ¥ì‚¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            info_data = sheets_processor.read_info_sheet(spreadsheet_id)
            if info_data is None:
                continue

            # WORKSHEET ì‹œíŠ¸ì—ì„œ ì‘ì—… ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            worksheet_df = sheets_processor.read_sheet(spreadsheet_id)
            if worksheet_df is None:
                logger.error(f"âŒ WORKSHEET ë°ì´í„°ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {spreadsheet_id}")
                continue

            # ë°ì´í„° ë¶„ì„
            analyzer = TaskAnalyzer(info_data.get("ëª¨ë¸ëª…", "Unknown"))
            analyzed_df, task_total_time, category_sums, progress_summary = analyzer.analyze_tasks(worksheet_df)
            if analyzed_df is None:
                logger.error(f"âŒ ë°ì´í„° ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {spreadsheet_id}")
                continue

            processed_data.append(
                {
                    "spreadsheet_id": spreadsheet_id,
                    "model_name": info_data.get("ëª¨ë¸ëª…", "Unknown"),
                    "mechanical_partner": info_data.get("ê¸°êµ¬í˜‘ë ¥ì‚¬", "Unknown"),
                    "electrical_partner": info_data.get("ì „ì¥í˜‘ë ¥ì‚¬", "Unknown"),
                    "tasks": analyzed_df,
                    "task_total_time": task_total_time,
                    "category_sums": category_sums,
                    "progress_summary": progress_summary,
                }
            )

        if not processed_data:
            logger.error("âŒ ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        # ì‹œê°í™”
        chart_generator = HeatmapGenerator()

        # íˆíŠ¸ë§µ ìƒì„±
        heatmap_generator = HeatmapGenerator()
        for data in processed_data:
            heatmap_generator.create_heatmap(data["tasks"], data["model_name"])

        # ì§„í–‰ë¥  ì°¨íŠ¸ ìƒì„±
        chart_generator.create_progress_chart(processed_data)

        # íƒ€ì„ë¼ì¸ ì°¨íŠ¸ ìƒì„±
        for spreadsheet_id, data in processed_data:
            chart_generator.create_timeline_chart(data["tasks"])

        # HTML ë¦¬í¬íŠ¸ ìƒì„±
        report_generator = ReportGenerator()
        report_generator.generate_html_report(processed_data, "PDA_Report")

        # JSON íŒŒì¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_path = os.path.join("output", f"PDA_Report_data_{timestamp}.json")

        # int64ë¥¼ intë¡œ ë³€í™˜
        for spreadsheet_id, data in processed_data:
            tasks_dict = data["tasks"].to_dict("records")
            for task in tasks_dict:
                for key, value in task.items():
                    if isinstance(value, np.int64):
                        task[key] = int(value)
                    elif isinstance(value, pd.Timestamp):
                        task[key] = value.strftime("%Y-%m-%d %H:%M:%S")
            data["tasks"] = tasks_dict

        try:
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(processed_data, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {os.path.basename(json_path)}")
        except Exception as e:
            logger.error(f"âŒ JSON íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")

        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ í™•ì¸
        if args.test_mode:
            logger.info("ğŸ”§ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: íŒŒì¼ ìƒì„±ë§Œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            logger.info("ğŸ“ ìƒì„±ëœ íŒŒì¼ ëª©ë¡:")
            logger.info(f"  - data: {json_path}")
            return True

        # GitHubì— ì—…ë¡œë“œ
        github_uploader = GitHubUploader()
        github_uploader.upload_files()

        # Google Driveì— ì—…ë¡œë“œ
        drive_uploader = DriveUploader(drive_service)
        drive_uploader.upload_files()

        # ì•Œë¦¼ ì „ì†¡
        notifier = EmailNotifier()
        notifier.send_email(
            args.email_to.split(","),
            "PDA_Report ì‘ì—… í˜„í™© ë¦¬í¬íŠ¸",
            open("PDA_Report.html", "r", encoding="utf-8").read(),
            [f"PDA_Report_heatmap.png", f"PDA_Report_progress.png", f"PDA_Report_timeline.png"],
        )

        logger.info("âœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True

    except Exception as e:
        logger.error(f"âŒ ì‘ì—… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return False


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="PDA (Production Data Analysis) ì‹œìŠ¤í…œ")

    parser.add_argument("--spreadsheet-id", required=True, help="Google Spreadsheet ID")
    parser.add_argument("--range-name", required=False, help="ì‹œíŠ¸ ë²”ìœ„ (ê¸°ë³¸ê°’: í™˜ê²½ ë³€ìˆ˜ WORKSHEET_RANGE ì‚¬ìš©)")
    parser.add_argument(
        "--model-name", required=False, help="ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: ìŠ¤í”„ë ˆë“œì‹œíŠ¸ì˜ 'ì •ë³´íŒ'ì—ì„œ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜´)"
    )
    parser.add_argument(
        "--mech-partner", required=False, help="ê¸°êµ¬ í˜‘ë ¥ì‚¬ (ê¸°ë³¸ê°’: ìŠ¤í”„ë ˆë“œì‹œíŠ¸ì˜ 'ì •ë³´íŒ'ì—ì„œ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜´)"
    )
    parser.add_argument(
        "--elec-partner", required=False, help="ì „ì¥ í˜‘ë ¥ì‚¬ (ê¸°ë³¸ê°’: ìŠ¤í”„ë ˆë“œì‹œíŠ¸ì˜ 'ì •ë³´íŒ'ì—ì„œ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜´)"
    )
    parser.add_argument("--tolerance", type=float, default=2.0, help="Overtime íŒë‹¨ ê¸°ì¤€ (ê¸°ì¤€ ì‹œê°„ ëŒ€ë¹„ ë°°ìˆ˜)")
    parser.add_argument("--upload-github", action="store_true", help="GitHub ì—…ë¡œë“œ ì—¬ë¶€")
    parser.add_argument("--upload-drive", action="store_true", help="Google Drive ì—…ë¡œë“œ ì—¬ë¶€")
    parser.add_argument("--email", action="store_true", help="ì´ë©”ì¼ ì•Œë¦¼ ì—¬ë¶€")
    parser.add_argument("--email-to", help="ì´ë©”ì¼ ìˆ˜ì‹ ì (ì‰¼í‘œë¡œ êµ¬ë¶„)")
    parser.add_argument("--kakao", action="store_true", help="ì¹´ì¹´ì˜¤í†¡ ì•Œë¦¼ ì—¬ë¶€")
    parser.add_argument("--test-mode", action="store_true", help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰ (ì—…ë¡œë“œ ë° ì•Œë¦¼ ê¸°ëŠ¥ ë¹„í™œì„±í™”)")

    args = parser.parse_args()
    success = main(args)
    exit(0 if success else 1)
